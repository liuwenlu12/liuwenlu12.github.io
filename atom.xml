<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liuwenlu12.github.io/</id>
    <title>åˆ˜æ–‡è·¯çš„åšå®¢</title>
    <updated>2020-02-27T18:01:26.476Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liuwenlu12.github.io/"/>
    <link rel="self" href="https://liuwenlu12.github.io//atom.xml"/>
    <subtitle>ä¸å…¶ä»“çš‡è¿½èµ¶æ—¥è½ï¼Œä¸å¦‚é™å¾…æ¼«å¤©æ˜Ÿè¾°</subtitle>
    <logo>https://liuwenlu12.github.io//images/avatar.png</logo>
    <icon>https://liuwenlu12.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, åˆ˜æ–‡è·¯çš„åšå®¢</rights>
    <entry>
        <title type="html"><![CDATA[Spark Shuffle è§£æ]]></title>
        <id>https://liuwenlu12.github.io//post/spark-shuffle-jie-xi</id>
        <link href="https://liuwenlu12.github.io//post/spark-shuffle-jie-xi">
        </link>
        <updated>2020-02-27T17:07:56.000Z</updated>
        <content type="html"><![CDATA[<h4 id="æœªä¼˜åŒ–çš„hashshuffle">æœªä¼˜åŒ–çš„HashShuffle</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582823385827.png" alt=""></figure>
<p>ç¼ºç‚¹:</p>
<ol>
<li>map ä»»åŠ¡çš„ä¸­é—´ç»“æœé¦–å…ˆå­˜å…¥å†…å­˜(ç¼“å­˜), ç„¶åæ‰å†™å…¥ç£ç›˜. è¿™å¯¹äºå†…å­˜çš„å¼€é”€å¾ˆå¤§, å½“ä¸€ä¸ªèŠ‚ç‚¹ä¸Š map ä»»åŠ¡çš„è¾“å‡ºç»“æœé›†å¾ˆå¤§æ—¶, å¾ˆå®¹æ˜“å¯¼è‡´å†…å­˜ç´§å¼ , å‘ç”Ÿ OOM</li>
<li>ç”Ÿæˆå¾ˆå¤šçš„å°æ–‡ä»¶. å‡è®¾æœ‰ M ä¸ª MapTask, æœ‰ N ä¸ª ReduceTask, åˆ™ä¼šåˆ›å»º M * n ä¸ªå°æ–‡ä»¶, ç£ç›˜ I/O å°†æˆä¸ºæ€§èƒ½ç“¶é¢ˆ.</li>
</ol>
<h4 id="ä¼˜åŒ–çš„hashshuffle">ä¼˜åŒ–çš„HashShuffle</h4>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582823456943.png" alt=""></figure>
<p>ä¼˜åŒ–çš„ HashShuffle è¿‡ç¨‹å°±æ˜¯å¯ç”¨åˆå¹¶æœºåˆ¶ï¼Œåˆå¹¶æœºåˆ¶å°±æ˜¯å¤ç”¨bufferï¼Œå¼€å¯åˆå¹¶æœºåˆ¶çš„é…ç½®æ˜¯spark.shuffle.consolidateFilesã€‚è¯¥å‚æ•°é»˜è®¤å€¼ä¸ºfalseï¼Œå°†å…¶è®¾ç½®ä¸ºtrueå³å¯å¼€å¯ä¼˜åŒ–æœºåˆ¶ã€‚é€šå¸¸æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨HashShuffleManagerï¼Œé‚£ä¹ˆéƒ½å»ºè®®å¼€å¯è¿™ä¸ªé€‰é¡¹ã€‚</p>
<h4 id="sortshuffle-è§£æ">SortShuffle è§£æ</h4>
<p>æ™®é€š SortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823873456.png" alt=""></p>
<p>bypassSortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823904849.png" alt=""></p>
<p>bypassè¿è¡Œæœºåˆ¶çš„è§¦å‘æ¡ä»¶å¦‚ä¸‹(å¿…é¡»åŒæ—¶æ»¡è¶³)ï¼š</p>
<ol>
<li>shuffle map taskæ•°é‡å°äºspark.shuffle.sort.bypassMergeThresholdå‚æ•°çš„å€¼ï¼Œé»˜è®¤ä¸º200ã€‚</li>
<li>ä¸æ˜¯èšåˆç±»çš„shuffleç®—å­(æ²¡æœ‰é¢„èšåˆ)ï¼ˆæ¯”å¦‚groupByKeyï¼‰ã€‚</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark ä»»åŠ¡è°ƒåº¦æœºåˆ¶]]></title>
        <id>https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi</id>
        <link href="https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi">
        </link>
        <updated>2020-02-27T14:44:47.000Z</updated>
        <content type="html"><![CDATA[<p>å½“ Driver èµ·æ¥åï¼ŒDriver åˆ™ä¼šæ ¹æ®ç”¨æˆ·ç¨‹åºé€»è¾‘å‡†å¤‡ä»»åŠ¡ï¼Œå¹¶æ ¹æ®Executorèµ„æºæƒ…å†µé€æ­¥åˆ†å‘ä»»åŠ¡ã€‚<br>
åœ¨è¯¦ç»†é˜è¿°ä»»åŠ¡è°ƒåº¦å‰ï¼Œé¦–å…ˆè¯´æ˜ä¸‹ Spark é‡Œçš„å‡ ä¸ªæ¦‚å¿µã€‚ä¸€ä¸ª Spark åº”ç”¨ç¨‹åºåŒ…æ‹¬Jobã€Stageä»¥åŠTaskä¸‰ä¸ªæ¦‚å¿µï¼š</p>
<ol>
<li>Job æ˜¯ä»¥ Action ç®—å­ä¸ºç•Œï¼Œé‡åˆ°ä¸€ä¸ªActionç®—å­åˆ™è§¦å‘ä¸€ä¸ªJobï¼›</li>
<li>Stage æ˜¯ Job çš„å­é›†ï¼Œä»¥ RDD å®½ä¾èµ–(å³ Shuffle )ä¸ºç•Œï¼Œé‡åˆ° Shuffle åšä¸€æ¬¡åˆ’åˆ†ï¼›</li>
<li>Task æ˜¯ Stage çš„å­é›†ï¼Œä»¥å¹¶è¡Œåº¦(åˆ†åŒºæ•°)æ¥è¡¡é‡ï¼Œè¿™ä¸ª Stage åˆ†åŒºæ•°æ˜¯å¤šå°‘ï¼Œåˆ™è¿™ä¸ªStage å°±æœ‰å¤šå°‘ä¸ª Taskã€‚<br>
Spark çš„ä»»åŠ¡è°ƒåº¦æ€»ä½“æ¥è¯´åˆ†ä¸¤è·¯è¿›è¡Œï¼Œä¸€è·¯æ˜¯ Stage çº§çš„è°ƒåº¦ï¼Œä¸€è·¯æ˜¯ Task çº§çš„è°ƒåº¦ï¼Œæ€»ä½“è°ƒåº¦æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582816815462.png" alt=""></figure>
<p>Spark RDD é€šè¿‡å…¶ Transactions æ“ä½œï¼Œå½¢æˆäº†RDDè¡€ç¼˜å…³ç³»å›¾ï¼Œå³DAGï¼Œæœ€åé€šè¿‡Actionçš„è°ƒç”¨ï¼Œè§¦å‘Jobå¹¶è°ƒåº¦æ‰§è¡Œã€‚<br>
DAGSchedulerè´Ÿè´£Stageçº§çš„è°ƒåº¦ï¼Œä¸»è¦æ˜¯å°†jobåˆ‡åˆ†æˆè‹¥å¹²Stagesï¼Œå¹¶å°†æ¯ä¸ªStageæ‰“åŒ…æˆTaskSetäº¤ç»™TaskSchedulerè°ƒåº¦ã€‚<br>
TaskSchedulerè´Ÿè´£Taskçº§çš„è°ƒåº¦ï¼Œå°†DAGSchedulerä¼ è¿‡æ¥çš„TaskSetæŒ‰ç…§æŒ‡å®šçš„è°ƒåº¦ç­–ç•¥åˆ†å‘åˆ°Executorä¸Šæ‰§è¡Œï¼Œè°ƒåº¦è¿‡ç¨‹ä¸­SchedulerBackendè´Ÿè´£æä¾›å¯ç”¨èµ„æºï¼Œå…¶ä¸­SchedulerBackendæœ‰å¤šç§å®ç°ï¼Œåˆ†åˆ«å¯¹æ¥ä¸åŒçš„èµ„æºç®¡ç†ç³»ç»Ÿã€‚</p>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582821511401.png" alt=""></figure>
<p>Driveråˆå§‹åŒ–SparkContextè¿‡ç¨‹ä¸­ï¼Œä¼šåˆ†åˆ«åˆå§‹åŒ–DAGSchedulerã€TaskSchedulerã€SchedulerBackendä»¥åŠHeartbeatReceiverï¼Œå¹¶å¯åŠ¨ SchedulerBackendä»¥åŠHeartbeatReceiverã€‚<br>
SchedulerBackendé€šè¿‡ApplicationMasterç”³è¯·èµ„æºï¼Œå¹¶ä¸æ–­ä»TaskSchedulerä¸­æ‹¿åˆ°åˆé€‚çš„Taskåˆ†å‘åˆ°Executoræ‰§è¡Œã€‚<br>
HeartbeatReceiverè´Ÿè´£æ¥æ”¶Executorçš„å¿ƒè·³ä¿¡æ¯ï¼Œç›‘æ§Executorçš„å­˜æ´»çŠ¶å†µï¼Œå¹¶é€šçŸ¥åˆ°TaskSchedulerã€‚</p>
<h3 id="spark-stage-çº§åˆ«è°ƒåº¦">Spark Stage çº§åˆ«è°ƒåº¦</h3>
<figure data-type="image" tabindex="3"><img src="https://liuwenlu12.github.io//post-images/1582821547270.png" alt=""></figure>
<ol>
<li>Job ç”±æœ€ç»ˆçš„RDDå’ŒActionæ–¹æ³•å°è£…è€Œæˆï¼›</li>
<li>SparkContextå°†Jobäº¤ç»™DAGScheduleræäº¤ï¼Œå®ƒä¼šæ ¹æ®RDDçš„è¡€ç¼˜å…³ç³»æ„æˆçš„DAGè¿›è¡Œåˆ‡åˆ†ï¼Œå°†ä¸€ä¸ªJobåˆ’åˆ†ä¸ºè‹¥å¹²Stagesï¼Œå…·ä½“åˆ’åˆ†ç­–ç•¥æ˜¯ï¼Œç”±æœ€ç»ˆçš„RDDä¸æ–­é€šè¿‡ä¾èµ–å›æº¯åˆ¤æ–­çˆ¶ä¾èµ–æ˜¯å¦æ˜¯å®½ä¾èµ–ï¼Œå³ä»¥Shuffleä¸ºç•Œï¼Œåˆ’åˆ†Stageï¼Œçª„ä¾èµ–çš„RDDä¹‹é—´è¢«åˆ’åˆ†åˆ°åŒä¸€ä¸ªStageä¸­ï¼Œå¯ä»¥è¿›è¡Œpipelineå¼çš„è®¡ç®—ã€‚</li>
<li>åˆ’åˆ†çš„Stagesåˆ†ä¸¤ç±»ï¼Œä¸€ç±»å«åšResultStageï¼Œä¸ºDAGæœ€ä¸‹æ¸¸çš„Stageï¼Œç”±Actionæ–¹æ³•å†³å®šï¼Œå¦ä¸€ç±»å«åšShuffleMapStageï¼Œä¸ºä¸‹æ¸¸Stageå‡†å¤‡æ•°æ®</li>
</ol>
<h3 id="spark-task-çº§åˆ«è°ƒåº¦">Spark Task çº§åˆ«è°ƒåº¦</h3>
<p>TaskSetManagerç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚<br>
<img src="https://liuwenlu12.github.io//post-images/1582821601280.png" alt=""></p>
<figure data-type="image" tabindex="4"><img src="https://liuwenlu12.github.io//post-images/1582821628549.png" alt=""></figure>
<h3 id="fifoè°ƒåº¦ç­–ç•¥">FIFOè°ƒåº¦ç­–ç•¥</h3>
<p>å¦‚æœæ˜¯é‡‡ç”¨FIFOè°ƒåº¦ç­–ç•¥ï¼Œåˆ™ç›´æ¥ç®€å•åœ°å°†TaskSetManageræŒ‰ç…§å…ˆæ¥å…ˆåˆ°çš„æ–¹å¼å…¥é˜Ÿï¼Œå‡ºé˜Ÿæ—¶ç›´æ¥æ‹¿å‡ºæœ€å…ˆè¿›é˜Ÿçš„TaskSetManagerï¼Œå…¶æ ‘ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒTaskSetManagerä¿å­˜åœ¨ä¸€ä¸ªFIFOé˜Ÿåˆ—ä¸­ã€‚</p>
<figure data-type="image" tabindex="5"><img src="https://liuwenlu12.github.io//post-images/1582821681915.png" alt=""></figure>
<h3 id="fairè°ƒåº¦ç­–ç•¥08-å¼€å§‹æ”¯æŒ">FAIRè°ƒåº¦ç­–ç•¥(0.8 å¼€å§‹æ”¯æŒ)</h3>
<figure data-type="image" tabindex="6"><img src="https://liuwenlu12.github.io//post-images/1582821708143.png" alt=""></figure>
<p>FAIRæ¨¡å¼ä¸­æœ‰ä¸€ä¸ªrootPoolå’Œå¤šä¸ªå­Poolï¼Œå„ä¸ªå­Poolä¸­å­˜å‚¨ç€æ‰€æœ‰å¾…åˆ†é…çš„TaskSetMagagerã€‚<br>
åœ¨FAIRæ¨¡å¼ä¸­ï¼Œéœ€è¦å…ˆå¯¹å­Poolè¿›è¡Œæ’åºï¼Œå†å¯¹å­Poolé‡Œé¢çš„TaskSetMagagerè¿›è¡Œæ’åºï¼Œå› ä¸ºPoolå’ŒTaskSetMagageréƒ½ç»§æ‰¿äº†Schedulableç‰¹è´¨ï¼Œå› æ­¤ä½¿ç”¨ç›¸åŒçš„æ’åºç®—æ³•ã€‚<br>
æ’åºè¿‡ç¨‹çš„æ¯”è¾ƒæ˜¯åŸºäºFair-shareæ¥æ¯”è¾ƒçš„ï¼Œæ¯ä¸ªè¦æ’åºçš„å¯¹è±¡åŒ…å«ä¸‰ä¸ªå±æ€§: runningTaskså€¼ï¼ˆæ­£åœ¨è¿è¡Œçš„Taskæ•°ï¼‰ã€minShareå€¼ã€weightå€¼ï¼Œæ¯”è¾ƒæ—¶ä¼šç»¼åˆè€ƒé‡runningTaskså€¼ï¼ŒminShareå€¼ä»¥åŠweightå€¼ã€‚<br>
æ³¨æ„ï¼ŒminShareã€weightçš„å€¼å‡åœ¨å…¬å¹³è°ƒåº¦é…ç½®æ–‡ä»¶fairscheduler.xmlä¸­è¢«æŒ‡å®šï¼Œè°ƒåº¦æ± åœ¨æ„å»ºé˜¶æ®µä¼šè¯»å–æ­¤æ–‡ä»¶çš„ç›¸å…³é…ç½®ã€‚</p>
<ol>
<li>å¦‚æœ A å¯¹è±¡çš„runningTaskså¤§äºå®ƒçš„minShareï¼ŒB å¯¹è±¡çš„runningTaskså°äºå®ƒçš„minShareï¼Œé‚£ä¹ˆBæ’åœ¨Aå‰é¢ï¼›ï¼ˆrunningTasks æ¯” minShare å°çš„å…ˆæ‰§è¡Œï¼‰</li>
<li>å¦‚æœAã€Bå¯¹è±¡çš„ runningTasks éƒ½å°äºå®ƒä»¬çš„ minShareï¼Œé‚£ä¹ˆå°±æ¯”è¾ƒ runningTasks ä¸ math.max(minShare1, 1.0) çš„æ¯”å€¼ï¼ˆminShareä½¿ç”¨ç‡ï¼‰ï¼Œè°å°è°æ’å‰é¢ï¼›ï¼ˆminShareä½¿ç”¨ç‡ä½çš„å…ˆæ‰§è¡Œï¼‰</li>
<li>å¦‚æœAã€Bå¯¹è±¡çš„runningTaskséƒ½å¤§äºå®ƒä»¬çš„minShareï¼Œé‚£ä¹ˆå°±æ¯”è¾ƒrunningTasksä¸weightçš„æ¯”å€¼ï¼ˆæƒé‡ä½¿ç”¨ç‡ï¼‰ï¼Œè°å°è°æ’å‰é¢ã€‚ï¼ˆæƒé‡ä½¿ç”¨ç‡ä½çš„å…ˆæ‰§è¡Œï¼‰</li>
<li>å¦‚æœä¸Šè¿°æ¯”è¾ƒå‡ç›¸ç­‰ï¼Œåˆ™æ¯”è¾ƒåå­—ã€‚</li>
</ol>
<p>æ•´ä½“ä¸Šæ¥è¯´å°±æ˜¯é€šè¿‡minShareå’Œweightè¿™ä¸¤ä¸ªå‚æ•°æ§åˆ¶æ¯”è¾ƒè¿‡ç¨‹ï¼Œå¯ä»¥åšåˆ°è®©minShareä½¿ç”¨ç‡å’Œæƒé‡ä½¿ç”¨ç‡å°‘ï¼ˆå®é™…è¿è¡Œtaskæ¯”ä¾‹è¾ƒå°‘ï¼‰çš„å…ˆè¿è¡Œã€‚<br>
FAIRæ¨¡å¼æ’åºå®Œæˆåï¼Œæ‰€æœ‰çš„TaskSetManagerè¢«æ”¾å…¥ä¸€ä¸ªArrayBufferé‡Œï¼Œä¹‹åä¾æ¬¡è¢«å–å‡ºå¹¶å‘é€ç»™Executoræ‰§è¡Œã€‚<br>
ä»è°ƒåº¦é˜Ÿåˆ—ä¸­æ‹¿åˆ°TaskSetManageråï¼Œç”±äºTaskSetManagerå°è£…äº†ä¸€ä¸ªStageçš„æ‰€æœ‰Taskï¼Œå¹¶è´Ÿè´£ç®¡ç†è°ƒåº¦è¿™äº›Taskï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥çš„å·¥ä½œå°±æ˜¯TaskSetManageræŒ‰ç…§ä¸€å®šçš„è§„åˆ™ä¸€ä¸ªä¸ªå–å‡ºTaskç»™TaskSchedulerï¼ŒTaskSchedulerå†äº¤ç»™SchedulerBackendå»å‘åˆ°Executorä¸Šæ‰§è¡Œã€‚<br>
å¦‚ä½•å¯ç”¨å…¬å¹³è°ƒåº¦å™¨:</p>
<pre><code>val conf = new SparkConf().setMaster(...).setAppName(...)
conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)
val sc = new SparkContext(conf)
</code></pre>
<h3 id="æœ¬åœ°åŒ–è°ƒåº¦">æœ¬åœ°åŒ–è°ƒåº¦</h3>
<table>
<thead>
<tr>
<th>åç§°</th>
<th>è§£æ</th>
</tr>
</thead>
<tbody>
<tr>
<td>PROCESS_LOCAL</td>
<td>è¿›ç¨‹æœ¬åœ°åŒ–ï¼Œtaskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªExecutorä¸­ï¼Œæ€§èƒ½æœ€å¥½ã€‚</td>
</tr>
<tr>
<td>NODE_LOCAL</td>
<td>èŠ‚ç‚¹æœ¬åœ°åŒ–ï¼Œtaskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­ï¼Œä½†æ˜¯taskå’Œæ•°æ®ä¸åœ¨åŒä¸€ä¸ªExecutorä¸­ï¼Œæ•°æ®éœ€è¦åœ¨è¿›ç¨‹é—´è¿›è¡Œä¼ è¾“ã€‚</td>
</tr>
<tr>
<td>RACK_LOCAL</td>
<td>æœºæ¶æœ¬åœ°åŒ–ï¼Œtaskå’Œæ•°æ®åœ¨åŒä¸€ä¸ªæœºæ¶çš„ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šï¼Œæ•°æ®éœ€è¦é€šè¿‡ç½‘ç»œåœ¨èŠ‚ç‚¹ä¹‹é—´è¿›è¡Œä¼ è¾“ã€‚</td>
</tr>
<tr>
<td>NO_PREF</td>
<td>å¯¹äºtaskæ¥è¯´ï¼Œä»å“ªé‡Œè·å–éƒ½ä¸€æ ·ï¼Œæ²¡æœ‰å¥½åä¹‹åˆ†ã€‚</td>
</tr>
<tr>
<td>ANY</td>
<td>taskå’Œæ•°æ®å¯ä»¥åœ¨é›†ç¾¤çš„ä»»ä½•åœ°æ–¹ï¼Œè€Œä¸”ä¸åœ¨ä¸€ä¸ªæœºæ¶ä¸­ï¼Œæ€§èƒ½æœ€å·®ã€‚</td>
</tr>
</tbody>
</table>
<h3 id="å¤±è´¥é‡è¯•å’Œé»‘åå•">å¤±è´¥é‡è¯•å’Œé»‘åå•</h3>
<p>åœ¨è®°å½•Taskå¤±è´¥æ¬¡æ•°è¿‡ç¨‹ä¸­ï¼Œä¼šè®°å½•å®ƒä¸Šä¸€æ¬¡å¤±è´¥æ‰€åœ¨çš„Executor Idå’ŒHostï¼Œè¿™æ ·ä¸‹æ¬¡å†è°ƒåº¦è¿™ä¸ªTaskæ—¶ï¼Œä¼šä½¿ç”¨é»‘åå•æœºåˆ¶ï¼Œé¿å…å®ƒè¢«è°ƒåº¦åˆ°ä¸Šä¸€æ¬¡å¤±è´¥çš„èŠ‚ç‚¹ä¸Šï¼Œèµ·åˆ°ä¸€å®šçš„å®¹é”™ä½œç”¨ã€‚é»‘åå•è®°å½•Taskä¸Šä¸€æ¬¡å¤±è´¥æ‰€åœ¨çš„Executor Idå’ŒHostï¼Œä»¥åŠå…¶å¯¹åº”çš„â€œæ‹‰é»‘â€æ—¶é—´ï¼Œ<br>
â€œæ‹‰é»‘â€æ—¶é—´æ˜¯æŒ‡è¿™æ®µæ—¶é—´å†…ä¸è¦å†å¾€è¿™ä¸ªèŠ‚ç‚¹ä¸Šè°ƒåº¦è¿™ä¸ªTaskäº†ã€‚</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark éƒ¨ç½²æ¨¡å¼åŠæºç åˆ†æ]]></title>
        <id>https://liuwenlu12.github.io//post/spark-bu-shu-mo-shi</id>
        <link href="https://liuwenlu12.github.io//post/spark-bu-shu-mo-shi">
        </link>
        <updated>2020-02-27T08:13:12.000Z</updated>
        <content type="html"><![CDATA[<p>Sparkæ”¯æŒ3ç§é›†ç¾¤ç®¡ç†å™¨ï¼ˆCluster Managerï¼‰ï¼Œåˆ†åˆ«ä¸ºï¼š</p>
<ol>
<li>Standaloneï¼šç‹¬ç«‹æ¨¡å¼ï¼ŒSpark åŸç”Ÿçš„ç®€å•é›†ç¾¤ç®¡ç†å™¨ï¼Œè‡ªå¸¦å®Œæ•´çš„æœåŠ¡ï¼Œå¯å•ç‹¬éƒ¨ç½²åˆ°ä¸€ä¸ªé›†ç¾¤ä¸­ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å…¶ä»–èµ„æºç®¡ç†ç³»ç»Ÿï¼Œä½¿ç”¨ Standalone å¯ä»¥å¾ˆæ–¹ä¾¿åœ°æ­å»ºä¸€ä¸ªé›†ç¾¤ï¼›</li>
<li>Hadoop YARNï¼šç»Ÿä¸€çš„èµ„æºç®¡ç†æœºåˆ¶ï¼Œåœ¨ä¸Šé¢å¯ä»¥è¿è¡Œå¤šå¥—è®¡ç®—æ¡†æ¶ï¼Œå¦‚ MRã€Stormç­‰ã€‚æ ¹æ® Driver åœ¨é›†ç¾¤ä¸­çš„ä½ç½®ä¸åŒï¼Œåˆ†ä¸º yarn client å’Œ yarn clusterï¼›</li>
<li>Apache Mesosï¼šä¸€ä¸ªå¼ºå¤§çš„åˆ†å¸ƒå¼èµ„æºç®¡ç†æ¡†æ¶ï¼Œå®ƒå…è®¸å¤šç§ä¸åŒçš„æ¡†æ¶éƒ¨ç½²åœ¨å…¶ä¸Šï¼ŒåŒ…æ‹¬ Yarnã€‚</li>
</ol>
<h3 id="yarn-cluster-æ¨¡å¼">YARN Cluster æ¨¡å¼</h3>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582794556881.png" alt=""></figure>
<ol>
<li>æ‰§è¡Œè„šæœ¬æäº¤ä»»åŠ¡ï¼Œå®é™…æ˜¯å¯åŠ¨ä¸€ä¸ª SparkSubmit çš„ JVM è¿›ç¨‹ï¼›</li>
<li>SparkSubmit ç±»ä¸­çš„ mainæ–¹æ³•åå°„è°ƒç”¨Clientçš„mainæ–¹æ³•ï¼›</li>
<li>Clientåˆ›å»ºYarnå®¢æˆ·ç«¯ï¼Œç„¶åå‘Yarnå‘é€æ‰§è¡ŒæŒ‡ä»¤ï¼šbin/java ApplicationMasterï¼›</li>
<li>Yarnæ¡†æ¶æ”¶åˆ°æŒ‡ä»¤åä¼šåœ¨æŒ‡å®šçš„NMä¸­å¯åŠ¨ApplicationMasterï¼›</li>
<li>ApplicationMasterå¯åŠ¨Driverçº¿ç¨‹ï¼Œæ‰§è¡Œç”¨æˆ·çš„ä½œä¸šï¼›</li>
<li>AMå‘RMæ³¨å†Œï¼Œç”³è¯·èµ„æºï¼›</li>
<li>è·å–èµ„æºåAMå‘NMå‘é€æŒ‡ä»¤ï¼šbin/java CoarseGrainedExecutorBackenï¼›</li>
<li>å¯åŠ¨ExecutorBackend, å¹¶å‘driveræ³¨å†Œ.</li>
<li>æ³¨å†ŒæˆåŠŸå, ExecutorBackendä¼šåˆ›å»ºä¸€ä¸ªExecutorå¯¹è±¡.</li>
<li>Driverä¼šç»™ExecutorBackendåˆ†é…ä»»åŠ¡, å¹¶ç›‘æ§ä»»åŠ¡çš„æ‰§è¡Œ.<br>
æ³¨æ„:<br>
â€¢	SparkSubmitã€ApplicationMasterå’ŒCoarseGrainedExecutorBackenæ˜¯ç‹¬ç«‹çš„è¿›ç¨‹ï¼›<br>
â€¢	Clientå’ŒDriveræ˜¯ç‹¬ç«‹çš„çº¿ç¨‹ï¼›<br>
â€¢	Executoræ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚</li>
</ol>
<pre><code>cluster: 
bin/spark-submit \                                             
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.11-2.1.1.jar 1000

-------
SparkSubmit
ApplicationMaster
CoarseGrainedExecutorBackend
--------

SparkSubmit
    org.apache.spark.deploy.SparkSubmit


    if (deployMode == CLIENT || isYarnCluster) {
            // å¦‚æœæ˜¯å®¢æˆ·ç«¯æ¨¡å¼, childMainClass å°±æ˜¯ç”¨æˆ·çš„ç±»
            // é›†ç¾¤æ¨¡å¼ä¸‹, childMainClass è¢«é‡æ–°èµ‹å€¼ä¸º org.apache.spark.deploy.yarn.Client
            childMainClass = args.mainClass  // ç”¨æˆ·ä¸»ç±»
            if (isUserJar(args.primaryResource)) {
                childClasspath += args.primaryResource
            }
            if (args.jars != null) {
                childClasspath ++= args.jars.split(&quot;,&quot;)
            }
        }
    if (isYarnCluster) {
            // åœ¨ yarn é›†ç¾¤æ¨¡å¼ä¸‹, ä½¿ç”¨yarn.Clientæ¥å°è£…ä¸€ä¸‹ user class
            childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
            if (args.isPython) {
                childArgs += (&quot;--primary-py-file&quot;, args.primaryResource)
                childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.PythonRunner&quot;)
            } else if (args.isR) {
                val mainFile = new Path(args.primaryResource).getName
                childArgs += (&quot;--primary-r-file&quot;, mainFile)
                childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.RRunner&quot;)
            } else {
                if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
                    childArgs += (&quot;--jar&quot;, args.primaryResource)
                }
                childArgs += (&quot;--class&quot;, args.mainClass)
            }
            if (args.childArgs != null) {
                args.childArgs.foreach { arg =&gt; childArgs += (&quot;--arg&quot;, arg) }
            }
        }

        childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
            submitApplication()

        // æ ¸å¿ƒä»£ç : ç¡®å®š ApplicationMaster ç±»
        val amClass =
            if (isClusterMode) { // å¦‚æœæ˜¯ cluster æ¨¡å¼
                Utils.classForName(&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;).getName
            } else { // å¦‚æœæ˜¯ client æ¨¡å¼
                Utils.classForName(&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;).getName
            }

        yarnClient.submitApplication(appContext)

        spark-submitè¿›ç¨‹æ‰§è¡Œå®Œæ¯•

ApplicationMaster
        runDriver()
        æ³¨å†ŒAM è®©NMè¦å¯åŠ¨çš„ç±»
        org.apache.spark.executor.CoarseGrainedExecutorBackend

        æ‰§è¡Œå®Œæ¯•, ä½†æ˜¯AMå¹¶ä¸ä¼šé€€å‡º,  dirverThread.join()
CoarseGrainedExecutorBackend
        executorè¿›ç¨‹
        // è·å–åˆ° driver çš„ RpcEndpointRef  (DriverEndpointæ˜¯åœ¨sparkContextä¸­åˆ›å»º)
            val driver: RpcEndpointRef = fetcher.setupEndpointRefByURI(driverUrl)

        ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))

        driverç«¯:
            executorRef.send(RegisteredExecutor)
        executorç«¯:
            executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)

</code></pre>
<blockquote>
<p>Cluster æ¨¡å¼æ¦‚è¿°ï¼šè„šæœ¬åœ¨å®¢æˆ·ç«¯ä¸Šæäº¤æ‰§è¡Œä¹‹åï¼Œæ‰§è¡ŒSparkSubmitçš„mainæ–¹æ³•ï¼Œmainæ–¹æ³•ä¸­ä¼šåå°„ä¸€ä¸ªcilentç±»ï¼Œæ‰§è¡Œclientçš„mainæ–¹æ³•ï¼Œå°è£…ä¸€ä¸ªæŒ‡ä»¤(bin/java Application)äº¤ç»™RM,RMä¼šé€‰æ‹©ä¸€å°NodeManagerå¯åŠ¨AM,AMä¼šè¿è¡ŒDriver(è¿è¡Œç”¨æˆ·ç±»çš„mainæ–¹æ³•),åŒæ—¶AMä¼šå‘RMç”³è¯·èµ„æºå’Œå®¹å™¨,å°è£…å¹¶ä¸”å‘é€ä¸€ä¸ªæŒ‡ä»¤(bin/java CoarseGrainedExecutorBackend),è®©å…¶ä»–NodeManagerå¯åŠ¨ExecutorBackendè¿›ç¨‹,ExecutorBackendè¿›ç¨‹ä¼šå‘Driverè¿›è¡Œåå‘æ³¨å†Œ(RegisterExecutor),Driverè¿”å›ä¸€ä¸ªæˆåŠŸæ³¨å†Œçš„ä¿¡æ¯,Executorè¿›ç¨‹ä¼šåˆ›å»ºä¸€ä¸ªExecutorå¯¹è±¡,ä¹‹åå°±å¯ä»¥è¿›è¡Œä»»åŠ¡åˆ†é…</p>
</blockquote>
<p>SparkSubmit  Application ExecutorBackendä¸‰ä¸ªè¿›ç¨‹</p>
<h3 id="yarn-client-æ¨¡å¼">Yarn Client æ¨¡å¼</h3>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582800089613.png" alt=""></figure>
<ol>
<li>æ‰§è¡Œè„šæœ¬æäº¤ä»»åŠ¡ï¼Œå®é™…æ˜¯å¯åŠ¨ä¸€ä¸ªSparkSubmitçš„ JVM è¿›ç¨‹ï¼›</li>
<li>SparkSubmitä¼´ç”Ÿå¯¹è±¡ä¸­çš„mainæ–¹æ³•åå°„è°ƒç”¨ç”¨æˆ·ä»£ç çš„mainæ–¹æ³•ï¼›</li>
<li>å¯åŠ¨Driverçº¿ç¨‹ï¼Œæ‰§è¡Œç”¨æˆ·çš„ä½œä¸šï¼Œå¹¶åˆ›å»ºScheduleBackendï¼›</li>
<li>YarnClientSchedulerBackendå‘RMå‘é€æŒ‡ä»¤ï¼šbin/java ExecutorLauncherï¼›</li>
<li>Yarnæ¡†æ¶æ”¶åˆ°æŒ‡ä»¤åä¼šåœ¨æŒ‡å®šçš„NMä¸­å¯åŠ¨ExecutorLauncherï¼ˆå®é™…ä¸Šè¿˜æ˜¯è°ƒç”¨ApplicationMasterçš„mainæ–¹æ³•ï¼‰ï¼›</li>
</ol>
<pre><code>object ExecutorLauncher {
  def main(args: Array[String]): Unit = {
    ApplicationMaster.main(args)
  }
}
</code></pre>
<ol start="6">
<li>AMå‘RMæ³¨å†Œï¼Œç”³è¯·èµ„æºï¼›</li>
<li>è·å–èµ„æºåAMå‘NMå‘é€æŒ‡ä»¤ï¼šbin/java CoarseGrainedExecutorBackenï¼›</li>
<li>åé¢å’Œclusteræ¨¡å¼ä¸€è‡´<br>
æ³¨æ„ï¼š</li>
</ol>
<ul>
<li>SparkSubmitã€ExecutorLauncherå’ŒCoarseGrainedExecutorBackenæ˜¯ç‹¬ç«‹çš„è¿›ç¨‹ï¼›</li>
<li>driverä¸æ˜¯ä¸€ä¸ªå­çº¿ç¨‹,è€Œæ˜¯ç›´æ¥è¿è¡Œåœ¨SparkSubmitè¿›ç¨‹çš„mainçº¿ç¨‹ä¸­, æ‰€ä»¥sparkSubmitè¿›ç¨‹ä¸èƒ½é€€å‡º.</li>
</ul>
<pre><code>bin/spark-submit \                                            
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.11-2.1.1.jar 1000

------
SparkSubmit
ExecutorLauncher  (am)
CoarseGrainedExecutorBackend

SparkSubmit
    childMainClass = args.mainClass  // ç”¨æˆ·ä¸»ç±»

    æ‰§è¡Œç”¨æˆ·ç±»çš„mainæ–¹æ³•(æ‰§è¡Œdriver)
        æ˜¯åœ¨SparkSubmitçš„ä¸»çº¿ç¨‹ä¸­æ‰§è¡Œ
        (cluster æ¨¡å¼driverè·Ÿç€: am, amå¯åŠ¨äº†ä¸€ä¸ªå­çº¿ç¨‹)

    ç”¨æˆ·ç±»é‡Œé¢, é¦–å…ˆå°±æ˜¯åœ¨åˆ›å»ºå’Œåˆå§‹åŒ–SaprkContext

    SparkContextçš„åˆå§‹åŒ–:
        val (sched, ts): (SchedulerBackend, TaskScheduler) = SparkContext.createTaskScheduler(this, master, deployMode)
        
        _ts.start()
            backend.start()  
            CoarseGrainedSchedulerBackend: start
                driverEndpoint = createDriverEndpointRef(properties)

            client.submitApplication

                org.apache.spark.deploy.yarn.ExecutorLauncher
                ä¸ºä»€ä¹ˆè¦æ¢ä¸ªåå­—ï¼šç›®çš„æ˜¯ä¸ºäº†åŒºåˆ†æ˜¯è¿è¡Œçš„clientè¿˜æ˜¯clusteræ¨¡å¼

            å¯åŠ¨DrvierEndPoint
                makeOffers
                    launchTasks(scheduler.resourceOffers(workOffers))

</code></pre>
<blockquote>
<p>Clientæ¨¡å¼æ¦‚è¿°ï¼šè„šæœ¬submitæäº¤ï¼Œè„šæœ¬å¯åŠ¨ï¼Œæ‰§è¡ŒSparkSubmitçš„mainæ–¹æ³•(åŒæ—¶Driverä¹Ÿåœ¨SparkSubmitä¸»çº¿ç¨‹ä¸­å¯åŠ¨)ï¼ŒDriverçš„æ‰§è¡Œä¼šåˆå§‹åŒ–SchedulerBackend, TaskSchedulerï¼Œå…¶ä¸­çš„YarnCilentSchedulerBackendä¸­æœ‰ä¸ªstartæ–¹æ³•å°è£…ä¸€ä¸ªbin/java ExcutorLauncheræŒ‡ä»¤äº¤ç»™RM,RMä¼šæ‰¾ä¸€ä¸ªNodeManageræ‰§è¡ŒExcutorLauncher(AM),ExcutorLauncherå¯åŠ¨åä¼šå‘RMç”³è¯·èµ„æºï¼Œå®¹å™¨.AMå°è£…å‘é€æŒ‡ä»¤ï¼Œ ç„¶åä¼šæ ¹æ®èµ„æºå’Œå®¹å™¨å»åˆ†é…NodeManagerå¯åŠ¨ExcutorBackendè¿›ç¨‹ï¼ŒExcutorBackendè¿›ç¨‹ä¼šå‘è¿›è¡ŒDriveræ³¨å†Œ(ä½¿ç”¨RegisterExecutor),è¿”å›æ³¨å†ŒæˆåŠŸä¹‹åï¼Œåˆ›å»ºä¸€ä¸ªExecutorå¯¹è±¡ï¼Œå°±å¯ä»¥åˆ†é…ä»»åŠ¡ã€‚</p>
</blockquote>
<h3 id="clientæ¨¡å¼-å’Œ-clusteræ¨¡å¼çš„åŒºåˆ«">Clientæ¨¡å¼ å’Œ Clusteræ¨¡å¼çš„åŒºåˆ«</h3>
<pre><code>1. spark-submitä¸­åå°„childMainClassä¸åŒ
    cluster:Client    childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
    client:ç”¨æˆ·ç±»      childMainClass = args.mainClass
2. driverä½ç½®
    cluster:ä½äºApplicationMasterè¿›ç¨‹ä¸­ï¼Œèµ·ä¸€ä¸ªå­çº¿ç¨‹
    client:ä½äºSparkSubmitçš„ä¸»çº¿ç¨‹
3. å¯åŠ¨AM
    cluster:ApplicationMaster
    client:ExecutorLauncher

  åœ¨clusteræ¨¡å¼ä¸‹ï¼Œå¦‚æœAMä¸€æ—¦å¯åŠ¨ï¼ŒSparkSubmitè¿˜æœ‰ç”¨å—ï¼Ÿ   æ²¡æœ‰ç”¨ å¯ä»¥æ€æ‰ ä¸€ç›´åœ¨æ‰“å°æ—¥å¿—
  åœ¨clientæ¨¡å¼ä¸‹ ï¼Œä¸èƒ½æ€  æ€äº†Driverä¹Ÿæ²¡äº†
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ç¬”è®°]]></title>
        <id>https://liuwenlu12.github.io//post/bi-ji</id>
        <link href="https://liuwenlu12.github.io//post/bi-ji">
        </link>
        <updated>2020-02-26T16:08:30.000Z</updated>
        <content type="html"><![CDATA[<p>Â·Â·Â·<br>
saveAsTable:<br>
1. å¦‚æœè¡¨ä¸å­˜åœ¨, åˆ™ä¼šè‡ªåŠ¨åˆ›å»º<br>
2. å¦‚æœè¡¨å­˜åœ¨(append),  åˆ™åé¢å­˜å‚¨çš„æ•°æ®, å…ƒæ•°æ®çš„é¡ºåºè¦ä¸è¡¨ä¸­çš„å…ƒæ•°æ®çš„é¡ºåºä¸€è‡´<br>
è¡¨å†…æ•°æ®:<br>
a(int)     b(string)<br>
æ–°çš„æ•°æ®:<br>
aa(int)    bb(string)<br>
3. åˆ—åå¯ä»¥ä¸ä¸€è‡´</p>
<p>insertInto<br>
1. è¦æ±‚è¡¨å¿…é¡»å­˜åœ¨<br>
2. è¦æ±‚åˆ—åå¿…é¡»ä¸€è‡´.<br>
è¡¨å†…æ•°æ®:<br>
a(int)  b(String)</p>
<pre><code>    æ–°çš„æ•°æ®:
        b(string)  a(int)
</code></pre>
<p>Â·Â·Â·</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Sparké›†ç¾¤å¯åŠ¨æµç¨‹åˆ†æ]]></title>
        <id>https://liuwenlu12.github.io//post/spark-ji-qun-qi-dong-liu-cheng-fen-xi</id>
        <link href="https://liuwenlu12.github.io//post/spark-ji-qun-qi-dong-liu-cheng-fen-xi">
        </link>
        <updated>2020-02-26T15:53:46.000Z</updated>
        <content type="html"><![CDATA[<h4 id="æµç¨‹">æµç¨‹</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582732625980.png" alt=""></figure>
<h4 id="masteræœåŠ¡ç«¯å£-7077">MasteræœåŠ¡ç«¯å£ 7077</h4>
<pre><code>   1 val args = new MasterArguments(argStrings, conf)  å°è£…å‚æ•°

  2  val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)   å¯åŠ¨RPCï¼ŒEndpoint

 3 val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) åˆ›å»ºrpcEnv

4     new NettyRpcEnvFactory().create(config) //Netty

5    if (!config.clientMode) //é›†ç¾¤æ¨¡å¼ä¸ºtrue

6           Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1  å¯åŠ¨æœåŠ¡NettyRPCEnv

7           val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME,
      new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) å¯åŠ¨Endpoint

8      dispatcher.registerRpcEndpoint(name, endpoint) //æ³¨å†Œåˆ°dispatcher
            æ¶ˆæ¯åˆ†å‘å™¨ æå‡å¼‚æ­¥èƒ½åŠ›
        
---------------------------------------------
private[spark] trait RpcEnvFactory {
 * An end point for the RPC that defines what functions to trigger given a message.
 *
 * It is guaranteed that `onStart`, `receive` and `onStop` will be called in sequence.
 *
 * The life-cycle of an endpoint is:
 *
 * constructor -&gt; onStart -&gt; receive* -&gt; onStop

* Note: `receive` can be called concurrently. If you want `receive` to be thread-safe, please use
 * [[ThreadSafeRpcEndpoint]]
 *
 * If any error is thrown from one of [[RpcEndpoint]] methods except `onError`, `onError` will be
 * invoked with the cause. If `onError` throws an error, [[RpcEnv]] will ignore it.
---------------------------------------------
 9 onstart --&gt;
 checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate//å›ºå®šæ£€æŸ¥workeræ˜¯å¦è¶…æ—¶  é»˜è®¤æ¯åˆ†é’Ÿä¸€æ¬¡  è‡ªå·±ç»™è‡ªå·±å‘æ¶ˆæ¯         self.send(CheckForWorkerTimeOut)
 10 receive--&gt;
  override def receive: PartialFunction[Any, Unit] 
     case CheckForWorkerTimeOut =&gt;
      timeOutDeadWorkers()//ç§»é™¤worker
</code></pre>
<h4 id="worker">Worker</h4>
<pre><code> 1 val args = new WorkerArguments(argStrings, conf)  å°è£…å‚æ•°
    
 2 val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,
      args.memory, args.masters, args.workDir, conf = conf) å¯åŠ¨RPCï¼ŒEndpoint

  val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr) åˆ›å»ºrpcEnv

 val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_))  Msteråœ°å€(æ³¨å†Œç”¨  é«˜å¯ç”¨æœ‰å¤šä¸ª  éƒ½å‘é€) 

 rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory,
      masterAddresses, ENDPOINT_NAME, workDir, conf, securityMgr))   setupEndpoint

3 new Worker--&gt;

def assert(assertion: Boolean) {
    if (!assertion)
      throw new java.lang.AssertionError(&quot;assertion failed&quot;)  å·²æ³¨å†ŒæŠ›å¼‚å¸¸
  }

  4  registerWithMaster()  //æ³¨å†Œè‡ªå·±

    registerMasterFutures = tryRegisterAllMasters() //æ‰€æœ‰masteræ³¨å†Œ

     val masterEndpoint = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
     registerWithMaster(masterEndpoint)
        //è·å–ref æ³¨å†Œ

  masterEndpoint.ask[RegisterWorkerResponse] RegisterWorker(
      workerId, host, port, self, cores, memory, workerWebUiUrl))
 //å‘ä¿¡æ¯ å¸Œæœ›Msterå›ä¸€RegisterWorkerResponse  ç»™masterè‡ªå·±æ‰€æœ‰ä¿¡æ¯

  masterç«¯   override def receiveAndReply(context: RpcCallContext): PartialFunction 
     if (state == RecoveryState.STANDBY) {
        context.reply(MasterInStandby)   //è‡ªå·±STANDBY
....å„ç§äº¤äº’
           5   if (registerWorker(worker)) //Masterç»™ä¸æ³¨å†Œ

   workers.filter { w =&gt;
      (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == WorkerState.DEAD)
    }.foreach { w =&gt;
      workers -= w
    }   //æ›¾ç»æ³¨å†Œè¿‡ çŠ¶æ€æ­»  

  context.reply(RegisteredWorker(self, masterWebUiUrl))//å“åº”worker å‘Šè¯‰workeræˆ‘æ˜¯å“ªä¸ªmaster


logInfo(&quot;Successfully registered with master &quot; + masterRef.address.toSparkURL)
      registered = true
     changeMaster(masterRef, masterWebUiUrl) //æ”¹åœ°å€
forwordMessageScheduler.scheduleAtFixedRate //å‘å¿ƒè·³

      if (connected) { sendToMaster(Heartbeat(workerId, self)) }  //å¿ƒè·³
      case Some(masterRef) =&gt; masterRef.send(message)

         workerInfo.lastHeartbeat = System.currentTimeMillis() //Masterç«¯ ä¸€åˆ†é’Ÿè¶…æ—¶  åäº”ç§’å¿ƒè·³

masterå¦‚æœæ²¡èµ·æ¥   ç”¨å®šæ—¶å™¨å°è¯•ç»§ç»­æ³¨å†Œè‡ªå·±  16æ¬¡ï¼ˆ6+10ï¼‰

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparké€šè®¯æ¶æ„]]></title>
        <id>https://liuwenlu12.github.io//post/spark-tong-xun-jia-gou</id>
        <link href="https://liuwenlu12.github.io//post/spark-tong-xun-jia-gou">
        </link>
        <updated>2020-02-26T15:18:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="spark-å†…ç½®-rpc-æ¡†æ¶">Spark å†…ç½® RPC æ¡†æ¶</h4>
<p>åœ¨ Spark ä¸­, å¾ˆå¤šåœ°æ–¹éƒ½æ¶‰åŠåˆ°ç½‘ç»œé€šè®¯, æ¯”å¦‚ Spark å„ä¸ªç»„ä»¶é—´çš„æ¶ˆæ¯äº’é€š, ç”¨æˆ·æ–‡ä»¶ä¸ Jar åŒ…çš„ä¸Šä¼ , èŠ‚ç‚¹é—´çš„ Shuffle è¿‡ç¨‹, Block æ•°æ®çš„å¤åˆ¶ä¸å¤‡ä»½ç­‰.</p>
<ol>
<li>åœ¨ Spark0.x.x ä¸ Spark1.x.x ç‰ˆæœ¬ä¸­, ç»„ä»¶é—´çš„æ¶ˆæ¯é€šä¿¡ä¸»è¦å€ŸåŠ©äº Akka.</li>
<li>åœ¨ Spark1.3 ä¸­å¼•å…¥äº† Netty é€šä¿¡æ¡†æ¶. Akkaè¦æ±‚messageå‘é€ç«¯å’Œæ¥æ”¶ç«¯æœ‰ç›¸åŒçš„ç‰ˆæœ¬, æ‰€ä»¥ä¸ºäº†é¿å… Akka é€ æˆçš„ç‰ˆæœ¬é—®é¢˜ï¼Œå¹¶ç»™ç”¨æˆ·çš„åº”ç”¨æ›´å¤§çµæ´»æ€§ï¼Œå†³å®šä½¿ç”¨æ›´é€šç”¨çš„ RPC å®ç°ï¼Œä¹Ÿå°±æ˜¯ç°åœ¨çš„ Netty æ¥æ›¿ä»£ Akkaã€‚</li>
<li>Spark1.6 ä¸­ Akka å’Œ Netty å¯ä»¥é…ç½®ä½¿ç”¨ã€‚Netty å®Œå…¨å®ç°äº† Akka åœ¨Spark ä¸­çš„åŠŸèƒ½ã€‚</li>
<li>ä»Spark2.0.0, Akka è¢«ç§»é™¤.</li>
</ol>
<h4 id="actor-æ¨¡å‹">Actor æ¨¡å‹</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582730544913.png" alt=""></figure>
<h4 id="netty-é€šä¿¡æ¶æ„">Netty é€šä¿¡æ¶æ„</h4>
<p>Netty å€Ÿé‰´äº† Akka çš„ Actor æ¨¡å‹<br>
Sparké€šè®¯æ¡†æ¶ä¸­å„ä¸ªç»„ä»¶ï¼ˆClient/Master/Workerï¼‰å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªä¸ªç‹¬ç«‹çš„å®ä½“ï¼Œå„ä¸ªå®ä½“ä¹‹é—´é€šè¿‡æ¶ˆæ¯æ¥è¿›è¡Œé€šä¿¡ã€‚</p>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582730593467.png" alt=""></figure>
<p>è¯¦ç»†<br>
<img src="https://liuwenlu12.github.io//post-images/1582730972828.png" alt=""></p>
<p>é«˜å±‚ä¿¯è§†å›¾<br>
<img src="https://liuwenlu12.github.io//post-images/1582731230217.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparkå†…æ ¸]]></title>
        <id>https://liuwenlu12.github.io//post/spark-nei-he</id>
        <link href="https://liuwenlu12.github.io//post/spark-nei-he">
        </link>
        <updated>2020-02-26T14:43:04.000Z</updated>
        <content type="html"><![CDATA[<h3 id="sparkæ ¸å¿ƒç»„ä»¶">Sparkæ ¸å¿ƒç»„ä»¶</h3>
<ol>
<li>Cluster Manager (åˆ†é…çš„èµ„æºå±äºä¸€çº§åˆ†é…, å®ƒå°†å„ä¸ª Worker ä¸Šçš„å†…å­˜, CPU ç­‰èµ„æºåˆ†é…ç»™ Application, ä½†å¹¶ä¸è´Ÿè´£å¯¹ Executor çš„èµ„æºçš„åˆ†é…)
<ol>
<li>Master  (Standalone)</li>
<li>ResourceManager (Yarn)</li>
<li>MesosMaster (Mesos)</li>
<li>Kubernetes  2.3.0æ–°å¢ (dockeré…åˆ)</li>
</ol>
</li>
<li>Worker (å·¥ä½œèŠ‚ç‚¹ åœ¨ Yarn éƒ¨ç½²æ¨¡å¼ä¸‹å®é™…ç”± NodeManager æ›¿ä»£)
<ol>
<li>å°†è‡ªå·±çš„å†…å­˜, CPU ç­‰èµ„æºé€šè¿‡æ³¨å†Œæœºåˆ¶å‘ŠçŸ¥ Cluster Manager</li>
<li>åˆ›å»º Executorè¿›ç¨‹</li>
<li>å°†èµ„æºå’Œä»»åŠ¡è¿›ä¸€æ­¥åˆ†é…ç»™ Executor</li>
<li>åŒæ­¥èµ„æºä¿¡æ¯, Executor çŠ¶æ€ä¿¡æ¯ç»™ ClusterManager ç­‰.</li>
</ol>
</li>
<li>Driver (Spark é©±åŠ¨å™¨èŠ‚ç‚¹ï¼Œç”¨äºæ‰§è¡Œ Spark ä»»åŠ¡ä¸­çš„ main æ–¹æ³•ï¼Œè´Ÿè´£å®é™…ä»£ç çš„æ‰§è¡Œå·¥ä½œ)
<ol>
<li>å°†ç”¨æˆ·ç¨‹åºè½¬åŒ–ä¸ºä½œä¸šï¼ˆJobï¼‰ï¼›</li>
<li>åœ¨ Executor ä¹‹é—´è°ƒåº¦ä»»åŠ¡ï¼ˆTaskï¼‰ï¼›</li>
<li>è·Ÿè¸ª Executor çš„æ‰§è¡Œæƒ…å†µï¼›</li>
<li>é€šè¿‡ UI å±•ç¤ºæŸ¥è¯¢è¿è¡Œæƒ…å†µï¼›</li>
</ol>
</li>
<li>Executor (Spark Executor èŠ‚ç‚¹æ˜¯è´Ÿè´£åœ¨ Spark ä½œä¸šä¸­è¿è¡Œå…·ä½“ä»»åŠ¡ï¼Œä»»åŠ¡å½¼æ­¤ä¹‹é—´ç›¸äº’ç‹¬ç«‹)        Spark åº”ç”¨å¯åŠ¨æ—¶ï¼ŒExecutor èŠ‚ç‚¹è¢«åŒæ—¶å¯åŠ¨ï¼Œå¹¶ä¸”å§‹ç»ˆä¼´éšç€æ•´ä¸ª Spark åº”ç”¨çš„ç”Ÿå‘½å‘¨æœŸè€Œå­˜åœ¨ã€‚<br>
å¦‚æœæœ‰ Executor èŠ‚ç‚¹å‘ç”Ÿäº†æ•…éšœæˆ–å´©æºƒï¼ŒSpark åº”ç”¨ä¹Ÿå¯ä»¥ç»§ç»­æ‰§è¡Œï¼Œä¼šå°†å‡ºé”™èŠ‚ç‚¹ä¸Šçš„ä»»åŠ¡è°ƒåº¦åˆ°å…¶ä»– Executor èŠ‚ç‚¹ä¸Šç»§ç»­è¿è¡Œã€‚<br>
Executor æœ‰ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š</li>
<li>è´Ÿè´£è¿è¡Œç»„æˆ Spark åº”ç”¨çš„ä»»åŠ¡ï¼Œå¹¶å°†ç»“æœè¿”å›ç»™é©±åŠ¨å™¨ï¼ˆDriverï¼‰ï¼›</li>
<li>å®ƒä»¬é€šè¿‡è‡ªèº«çš„å—ç®¡ç†å™¨ï¼ˆBlock Managerï¼‰ä¸ºç”¨æˆ·ç¨‹åºä¸­è¦æ±‚ç¼“å­˜çš„ RDD æä¾›å†…å­˜å¼å­˜å‚¨ã€‚RDD çš„æ•°æ®æ˜¯ç›´æ¥ç¼“å­˜åœ¨ Executor è¿›ç¨‹å†…çš„ï¼Œå› æ­¤ä»»åŠ¡å¯ä»¥åœ¨è¿è¡Œæ—¶å……åˆ†åˆ©ç”¨ç¼“å­˜æ•°æ®åŠ é€Ÿè¿ç®—ã€‚</li>
<li>Application<br>
â€¢ Application é€šè¿‡ Spark API å°†è¿›è¡Œ RDD çš„è½¬æ¢å’Œ DAG çš„æ„å»º, å¹¶é€šè¿‡ Driver å°† Application æ³¨å†Œåˆ° Cluster Manager.<br>
â€¢	Cluster Manager å°†ä¼šæ ¹æ® Application çš„èµ„æºéœ€æ±‚, é€šè¿‡ä¸€çº§åˆ†é…å°† Executor, å†…å­˜, CPU ç­‰èµ„æºåˆ†é…ç»™ Application.<br>
â€¢	Driver é€šè¿‡äºŒçº§åˆ†é…å°† Executor ç­‰èµ„æºåˆ†é…ç»™æ¯ä¸€ä¸ªä»»åŠ¡, Application æœ€åé€šè¿‡ Driver å‘Šè¯‰ Executor è¿è¡Œä»»åŠ¡</li>
</ol>
<h5 id="4040ç«¯å£-driver">4040ç«¯å£-&gt;Driver</h5>
<h5 id="8080ç«¯å£-master">8080ç«¯å£-&gt;Master</h5>
<h5 id="8081ç«¯å£-worker">8081ç«¯å£-&gt;Worker</h5>
<h3 id="spark-é€šç”¨è¿è¡Œæµç¨‹æ¦‚è¿°">Spark é€šç”¨è¿è¡Œæµç¨‹æ¦‚è¿°</h3>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582730179336.png" alt=""></figure>
<ol>
<li>ä»»åŠ¡æäº¤åï¼Œéƒ½ä¼šå…ˆå¯åŠ¨ Driver ç¨‹åºï¼›</li>
<li>éšå Driver å‘é›†ç¾¤ç®¡ç†å™¨æ³¨å†Œåº”ç”¨ç¨‹åºï¼›</li>
<li>ä¹‹åé›†ç¾¤ç®¡ç†å™¨æ ¹æ®æ­¤ä»»åŠ¡çš„é…ç½®æ–‡ä»¶åˆ†é… Executor å¹¶å¯åŠ¨è¯¥åº”ç”¨ç¨‹åºï¼›</li>
<li>å½“ Driver æ‰€éœ€çš„èµ„æºå…¨éƒ¨æ»¡è¶³åï¼ŒDriver å¼€å§‹æ‰§è¡Œ main å‡½æ•°ï¼ŒSpark è½¬æ¢ä¸ºæ‡’æ‰§è¡Œï¼Œå½“æ‰§è¡Œåˆ° Action ç®—å­æ—¶å¼€å§‹åå‘æ¨ç®—ï¼Œæ ¹æ®å®½ä¾èµ–è¿›è¡Œ Stage çš„åˆ’åˆ†ï¼Œéšåæ¯ä¸€ä¸ª Stage å¯¹åº”ä¸€ä¸ª Tasksetï¼ŒTaskset ä¸­æœ‰å¤šä¸ªTaskï¼›</li>
<li>æ ¹æ®æœ¬åœ°åŒ–åŸåˆ™ï¼ŒTask ä¼šè¢«åˆ†å‘åˆ°æŒ‡å®šçš„ Executor å»æ‰§è¡Œï¼Œåœ¨ä»»åŠ¡æ‰§è¡Œçš„è¿‡ç¨‹ä¸­ï¼ŒExecutor ä¹Ÿä¼šä¸æ–­ä¸ Driver è¿›è¡Œé€šä¿¡ï¼ŒæŠ¥å‘Šä»»åŠ¡è¿è¡Œæƒ…å†µã€‚</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkStreamingå¤„ç†å®Œæ•°æ®å†™å…¥redis]]></title>
        <id>https://liuwenlu12.github.io//post/sparkstreaming-chu-li-wan-shu-ju-xie-ru-redis</id>
        <link href="https://liuwenlu12.github.io//post/sparkstreaming-chu-li-wan-shu-ju-xie-ru-redis">
        </link>
        <updated>2020-02-26T14:27:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="redisutil">RedisUtil</h4>
<pre><code>package com.atguigu.day02.project.util

import redis.clients.jedis.Jedis

object RedisUtil {
  /**
   * ä¸¤ç§æ–¹å¼ï¼š
   * 1.ä½¿ç”¨è¿æ¥æ± 
   * æ•ˆç‡æ›´é«˜ è¿æ¥ä¼šé‡ç”¨
   * å®é™…æƒ…å†µ å®¹æ˜“å‡ºç°å¤šçº¿ç¨‹bug
   * 2.æ‰‹åŠ¨åˆ›å»ºè¿æ¥çš„å®¢æˆ·ç«¯å¯¹è±¡
   * ç”¨å®ŒåŠ¡å¿…å…³é—­
   */
  val host = &quot;lwl007&quot;
  val port = 6379
    //è·å–å®¢æˆ·ç«¯å¯¹è±¡
  def getClient = new Jedis(host, port)
    //å…³é—­å®¢æˆ·ç«¯
  def close(client: Jedis): Unit = {
    if (client != null) client.close()
  }
}

</code></pre>
<h4 id="æ•°æ®å†™å…¥">æ•°æ®å†™å…¥</h4>
<pre><code>    import org.json4s.JsonDSL._ //scalaè‡ªå¸¦è½¬json
    import scala.collection.JavaConversions._ //scalaå’Œjavaé›†åˆäº’è½¬
    val key = &quot;last:hour:ads&quot;
    adsIdAndHmCountIt.foreachRDD(rdd =&gt; {
      rdd.foreachPartition((it: Iterator[(String, Iterable[(String, Int)])]) =&gt; {
        //å†™å…¥æ•°æ®
        if (it.hasNext) {
          //å»ºç«‹è¿æ¥
          val client: Jedis = RedisUtil.getClient
          val map: Map[String, String] = it.map {
            case (adsId, hmCountIt) =&gt; {
              //ä¸åŠ toMap [{},{}]  åŠ toMap{K1:V1,K2:V2}
              val hmCountItToMap: Map[String, Int] = hmCountIt.toMap
              (adsId, JsonMethods.compact(JsonMethods.render(hmCountItToMap)))
            }
          }.toMap
          client.hmset(key, map)
          RedisUtil.close(client)
        }
      })
    })
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[è¿­ä»£å™¨]]></title>
        <id>https://liuwenlu12.github.io//post/die-dai-qi</id>
        <link href="https://liuwenlu12.github.io//post/die-dai-qi">
        </link>
        <updated>2020-02-26T14:13:40.000Z</updated>
        <content type="html"><![CDATA[<h4 id="ä»–ä»¬æ˜¯ä¸€ç§æƒ°æ€§æ•°æ®ç»“æ„å¹¶ä¸”ä»–ä»¬æ•°æ®åªèƒ½ä½¿ç”¨ä¸€æ¬¡">ä»–ä»¬æ˜¯ä¸€ç§æƒ°æ€§æ•°æ®ç»“æ„ï¼Œå¹¶ä¸”ä»–ä»¬æ•°æ®åªèƒ½ä½¿ç”¨ä¸€æ¬¡</h4>
<pre><code> rdd.foreachPartition((it: Iterator[(String, Iterable[(String, Int)])]) =&gt;{
        //å»ºç«‹è¿æ¥
        val client: Jedis = RedisUtil.getClient
        //å†™å…¥æ•°æ®
        //Iterrator
        if(it.size&gt;0){
          println(it.toList)
        }
      })
</code></pre>
<h4 id="ç»“æœ">ç»“æœ</h4>
<pre><code>List()
List()
List()
List()
</code></pre>
<h4 id="è¿­ä»£å™¨ç‹¬æœ‰">è¿­ä»£å™¨ç‹¬æœ‰</h4>
<pre><code>  if(it.hasNext){  //it.next
          println(it.toList)
        }
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://liuwenlu12.github.io//post/hello-gridea</id>
        <link href="https://liuwenlu12.github.io//post/hello-gridea">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>ğŸ‘  æ¬¢è¿ä½¿ç”¨ <strong>Gridea</strong> ï¼<br>
âœï¸  <strong>Gridea</strong> ä¸€ä¸ªé™æ€åšå®¢å†™ä½œå®¢æˆ·ç«¯ã€‚ä½ å¯ä»¥ç”¨å®ƒæ¥è®°å½•ä½ çš„ç”Ÿæ´»ã€å¿ƒæƒ…ã€çŸ¥è¯†ã€ç¬”è®°ã€åˆ›æ„... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea ä¸»é¡µ</a><br>
<a href="http://fehey.com/">ç¤ºä¾‹ç½‘ç«™</a></p>
<h2 id="ç‰¹æ€§">ç‰¹æ€§ğŸ‘‡</h2>
<p>ğŸ“  ä½ å¯ä»¥ä½¿ç”¨æœ€é…·çš„ <strong>Markdown</strong> è¯­æ³•ï¼Œè¿›è¡Œå¿«é€Ÿåˆ›ä½œ</p>
<p>ğŸŒ‰  ä½ å¯ä»¥ç»™æ–‡ç« é…ä¸Šç²¾ç¾çš„å°é¢å›¾å’Œåœ¨æ–‡ç« ä»»æ„ä½ç½®æ’å…¥å›¾ç‰‡</p>
<p>ğŸ·ï¸  ä½ å¯ä»¥å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç»„</p>
<p>ğŸ“‹  ä½ å¯ä»¥è‡ªå®šä¹‰èœå•ï¼Œç”šè‡³å¯ä»¥åˆ›å»ºå¤–éƒ¨é“¾æ¥èœå•</p>
<p>ğŸ’»  ä½ å¯ä»¥åœ¨ <strong>Windows</strong>ï¼Œ<strong>MacOS</strong> æˆ– <strong>Linux</strong> è®¾å¤‡ä¸Šä½¿ç”¨æ­¤å®¢æˆ·ç«¯</p>
<p>ğŸŒ  ä½ å¯ä»¥ä½¿ç”¨ <strong>ğ–¦ğ—‚ğ—ğ—ğ—ğ–» ğ–¯ğ–ºğ—€ğ–¾ğ—Œ</strong> æˆ– <strong>Coding Pages</strong> å‘ä¸–ç•Œå±•ç¤ºï¼Œæœªæ¥å°†æ”¯æŒæ›´å¤šå¹³å°</p>
<p>ğŸ’¬  ä½ å¯ä»¥è¿›è¡Œç®€å•çš„é…ç½®ï¼Œæ¥å…¥ <a href="https://github.com/gitalk/gitalk">Gitalk</a> æˆ– <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> è¯„è®ºç³»ç»Ÿ</p>
<p>ğŸ‡¬ğŸ‡§  ä½ å¯ä»¥ä½¿ç”¨<strong>ä¸­æ–‡ç®€ä½“</strong>æˆ–<strong>è‹±è¯­</strong></p>
<p>ğŸŒ  ä½ å¯ä»¥ä»»æ„ä½¿ç”¨åº”ç”¨å†…é»˜è®¤ä¸»é¢˜æˆ–ä»»æ„ç¬¬ä¸‰æ–¹ä¸»é¢˜ï¼Œå¼ºå¤§çš„ä¸»é¢˜è‡ªå®šä¹‰èƒ½åŠ›</p>
<p>ğŸ–¥  ä½ å¯ä»¥è‡ªå®šä¹‰æºæ–‡ä»¶å¤¹ï¼Œåˆ©ç”¨ OneDriveã€ç™¾åº¦ç½‘ç›˜ã€iCloudã€Dropbox ç­‰è¿›è¡Œå¤šè®¾å¤‡åŒæ­¥</p>
<p>ğŸŒ± å½“ç„¶ <strong>Gridea</strong> è¿˜å¾ˆå¹´è½»ï¼Œæœ‰å¾ˆå¤šä¸è¶³ï¼Œä½†è¯·ç›¸ä¿¡ï¼Œå®ƒä¼šä¸åœå‘å‰ ğŸƒ</p>
<p>æœªæ¥ï¼Œå®ƒä¸€å®šä¼šæˆä¸ºä½ ç¦»ä¸å¼€çš„ä¼™ä¼´</p>
<p>å°½æƒ…å‘æŒ¥ä½ çš„æ‰åå§ï¼</p>
<p>ğŸ˜˜ Enjoy~</p>
]]></content>
    </entry>
</feed>