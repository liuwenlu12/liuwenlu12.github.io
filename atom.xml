<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liuwenlu12.github.io/</id>
    <title>刘文路的博客</title>
    <updated>2020-04-12T16:22:44.305Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liuwenlu12.github.io/"/>
    <link rel="self" href="https://liuwenlu12.github.io//atom.xml"/>
    <subtitle>与其仓皇追赶日落，不如静待漫天星辰</subtitle>
    <logo>https://liuwenlu12.github.io//images/avatar.png</logo>
    <icon>https://liuwenlu12.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, 刘文路的博客</rights>
    <entry>
        <title type="html"><![CDATA[hive]]></title>
        <id>https://liuwenlu12.github.io//post/hive</id>
        <link href="https://liuwenlu12.github.io//post/hive">
        </link>
        <updated>2020-04-12T16:21:30.000Z</updated>
        <content type="html"><![CDATA[<h4 id="组成">组成</h4>
<h4 id="hive与mysql区别">hive与mysql区别</h4>
<pre><code>            hive             mysql
数据量     大                  小
查询速度  （大数据快）        （小数据快）
语法      hql(底层是mr)     sql(底层是sql引擎)
        查询                 增删改查
</code></pre>
<h4 id="内部表与外部表区别送分">内部表与外部表区别（送分）</h4>
<pre><code>    原始数据 和元数据
    内部表删除数据：删除 原始数据 和元数据
    外部表删除数据：只删除元数据
    
    在开发时什么情况创建外部表，是否创建过内部表？
    多人共用的表，都是外部表。在生产环境绝大多数都是外部表。
    自己使用的临时表，用内部表。
</code></pre>
<h4 id="4个by">4个by</h4>
<pre><code>    order（全局排序）  sort（排序）  d（分区）  c（s和d字段相同时使用） 
    在开发时：
    order by：在企业里面用的不多，尤其是大企业。京东（OOM） 小公司数量不大，可以采用 
    sort+d：配合使用。在企业中用的最多。
    c:用的比较少。sort+d用他替代。
</code></pre>
<h4 id="系统函数">系统函数</h4>
<pre><code>    dateDiff year month
    日:date_sub date_add
    月：date_formart  last day
    周：next_day
    解析json函数   get json object  
    collect_set  concat_ws  concat
</code></pre>
<h4 id="自定义函数">自定义函数</h4>
<pre><code>    1）自定义UDF（一进一出）行为单位
        （1）解析公共字段
        （2）自定义UDF步骤
            定义类继承UDF，重新里面evaluate方法  String evaluate(key,line)
            打包/上传/创建永久函数（jar包上传到集群路径）
            创建完函数，发现jar有问题？  修改代码后再上传就OK
        
    2）自定义UDTF（一进多出，多进多出）30秒  实时推荐  页面退出
        （1）解析事件字段（点赞、评论、收藏）（商品列表、商品详情、商品点击）（广告）（故障）
        （2）自定义UDTF步骤
            定义类继承Gen..UDTF  重新三个方法（初始化、关闭、process）
            初始化方法，里面做了 定义返回值类型和名称
            process处理具体业务逻辑
    3）自定义UDF和UDTF我可以用get_json_object解析。为啥还要自定义？
        方便定位问题 debug调试,  处理复杂json会更简单
</code></pre>
<h4 id="窗口函数">窗口函数</h4>
<pre><code>    RANK
    over
    能够实现topn  现场手写
</code></pre>
<p>####优化<br>
1）mapjoin<br>
2）开启本地模式<br>
3）行列过滤  join   where=》where join<br>
4）创建分区表<br>
5）小文件处理<br>
（1）combinehiveinputformat  减少切片  1.txt  2.txt  3.txt =&gt;1个切片<br>
（2）JVM重用<br>
（3）merge<br>
6）合理设置map个数max(最小值 min(块大小，最大值))和reduce个数<br>
128m数据对应1g内存 1个maptask<br>
7）采用压缩<br>
8）采用列式存储<br>
9）开启map端combiner（不影响最终业务逻辑）</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kafka]]></title>
        <id>https://liuwenlu12.github.io//post/kafka</id>
        <link href="https://liuwenlu12.github.io//post/kafka">
        </link>
        <updated>2020-04-12T16:20:23.000Z</updated>
        <content type="html"><![CDATA[<p>kafka（20件事）</p>
<h4 id="基本信息">基本信息</h4>
<pre><code>    1）kafka组成
    2）安装多少台kafka： 2(生产者峰值的生产速率 * 副本数/100)+1=3
    3）采用压测（生产者峰值的生产速率）
    4）设置多少个副本：2-3个  2个的居多；  副本可以提供可靠性，但是会影响性能
    5）设置多少个topic；一张表一个topic，适度聚合也是可以的。
    6）数据默认保存多久：7天，  生产环境保存3天
    7）kafka做没做 监控器  km  kmo  eg  用的自己开发的
    8）isr leader挂了谁当老大，在isr里面的都可以当老大。 时间、条数； 新：时间
    9）分区分配策略：
        range（默认）多余任务发往低分区，容易导致数据倾斜
        10个任务  3  1-4  5-7 8-10
        10个任务 hash  10个序号，轮询
        
        randrobin：随机打散hash, 轮询，适当减少数据倾斜。
    10）数据量计算
        日活：100万  没人一天多条：100条   一天多少条日志=100万*100=1亿条
        平均每秒多少条日志=1亿条/(3600*24)=1150条/s
        1条日志大小多大：1kb（0.5-2k）
        平均每秒多少m = 1m/s
        
        峰值：   50m/s以内   10m-30m/s
        什么时候达到峰值：手纸 
    11）硬盘大小：
        每天的数据量（100g*2） * 3 天/0.7=
    12）分区数
        （1）创建一个只有1个分区的topic
        （2）测试这个topic的producer吞吐量和consumer吞吐量。
        （3）假设他们的值分别是Tp和Tc，单位可以是MB/s。
       （4）然后假设总的目标吞吐量是Tt，那么分区数=Tt / min（Tp，Tc）
        例如：producer吞吐量=20m/s；consumer吞吐量=50m/s，期望吞吐量100m/s；
        分区数=100 / 20 =5分区
        https://blog.csdn.net/weixin_42641909/article/details/89294698
        分区数一般设置为：3-10个
</code></pre>
<h4 id="挂了">挂了</h4>
<pre><code>    短时间内数据积压到flume channel
    长时间，可以再次消费日志服务器的数据
</code></pre>
<h4 id="丢了">丢了</h4>
<pre><code>    ack=0 1 -1
    0: 发送过去，就不管了        可靠性最低，效率最高
    1：发送过去，leader应答      可靠性一般，效率一般   
    -1：发送过去，leader和follower共同参与应答。    可靠性最高，效率最低
   在生产环境如何选择：
    不会选择0；
    普通的互联网行业，通常会选择1.
    金融、对钱比较敏感的数据：选择-1
</code></pre>
<h4 id="重复了">重复了</h4>
<pre><code>    幂等性+事务+ack=-1
    下级处理（hive dwd  streaming内部处理）(group by 开窗id取第一条)
</code></pre>
<h4 id="积压了">积压了</h4>
<pre><code>    （1）增加分区数，提高并发度， 分区数要和消费的CPU核数相等
    （2）调整每批次拉取的数据  100条/s =》1000条/s  1亿/s(内存溢出)
</code></pre>
<h4 id="优化">优化</h4>
<pre><code>    1）server
        （1）计算密集型（加减乘除运算、算法）         比较消耗CPU资源       CPU+1
        （2）IO密集型任务（数据进来，直接就出去了）   不消耗太多CPU资源    CPU*2
        （3）日志保存3天
        （4）副本设置为2
    2）生产者文件
        （1）配置压缩：none  gzip  snappy lz4
    3）内存 默认1g   生产环境调整到4-6个g ,比如配置了7g和6g效果一样。 可以考虑增加集群台数。
    4）Kafka 高效读写数据原因
        （1）kafka是集群，分布式
        （2）顺序读写 600m/s   随机读写100m/s
        （3）零拷贝 不经过应用层
    5） Kafka支持传输最大一条日志大小 
        默认1m   server.properties调整单条传输日志大小。
        
    6）Kafka过期数据清理  删除  压缩</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[flume]]></title>
        <id>https://liuwenlu12.github.io//post/flume</id>
        <link href="https://liuwenlu12.github.io//post/flume">
        </link>
        <updated>2020-04-12T16:18:34.000Z</updated>
        <content type="html"><![CDATA[<p>flume</p>
<h4 id="组成source-channel-sink-puttake事务">组成（source、channel、sink、put/take事务）</h4>
<pre><code>    1）taildir source
        （1）优点：断点续传、多目录
        （2）哪个版本产生的：Apache 1.7  CDH 1.6
        （3）三年前如何实现的断点续传？自定义source
        （4）taildir 挂了怎么办？
            重启就行吧，断点续传，不会丢数。 会导致数据重复
            一种是解决（自定义source，采用事务，
                    下一级处理（hive dwd /spark streaming））
                    group by id  开窗(id)，只取窗口第一个值
            二种不解决（要求不是特别准确的场景，普通的日志就没问题，金钱）
        （5）tail dir 支不支持，递归遍历文件夹，读取文件？  
             不支持。自定义source， 遍历文件夹+读文件
             自定义可以实现任何功能。
    2）channel
        （1）filechannel：基于磁盘，传输慢，安全
            100万个event
        （2）memorychannel：基于内存，传输快，不安全
            100个event
        （3）KafkaChannel：数据存储（Kafka，磁盘），
            传输快（memorychannel+kafkasink），安全， 只有下一级是Kafka才能用
            1.6出来的Kafkachannel，但是没火，有一个bug
            start_topic + 数据内容=&gt; 需要后续把start_topic去掉。
            提供对应参数，取消头，很遗憾，不起作用。
            1.7版本时解决了bug
        （4）生产环境怎么选择
            对可靠性要求比较高的行业（金融、对钱比较敏感的数据） filechannel 和Kafkachuannel
            对可靠性要求不高的行业（普通的互联网行业,普通日志）  memorychannel和Kafkachuannel
    3）sink
        (1)HDFS sink
            注意只要有hdfs的地方就要想到：小文件
            时间（1-2个小时）、大小（128m）、event个数(0)
            
    4）事务take put    
</code></pre>
<h4 id="三大器拦截器-channel选择器-监控器">三大器（拦截器、channel选择器、监控器）</h4>
<pre><code>    1）拦截器
    （1）ETL拦截器
        数据清洗
        清洗规则：清洗掉不少{开头 }结尾的数据。
        没有深入清洗的原因：影响性能。
        服务器时间的判断：长度必须是13位，  所有内容必须是数字           
    （2）分类型拦截器
        a)分了哪些类型
            启动日志
            事件日志（一张表一个topic,也会做适当的聚合）
            （商品列表、商品详情、商品点击；
            点赞、评论、收藏
            广告
            故障
            前台活跃、后台活跃、通知）
        b)可以不用拦截器吗
            可以， 如果不用的话，需要在hive的dwd层分类；或者再spark streaming内部处理
        c）自定义拦截器步骤
            定义一个类 实现interceptor接口，重写里面四个方法
            （初始化、单独event、批量event、关闭），创建一个静态内部类builder
            打包/上传/和配置文件关联
    2）channel选择器   
     （1）r（默认）  m
     （2）r特点：把数据发往下一级所有通道
          m特点：选择性发往下一级通道。
    3）监控器 
        （1）用什么对flume监控：Ganlia
        （2）put或take尝试提交的次数远远大于最终成功的次数
        （3）如果flume有问题怎么解决
            a）提高自己能力；增加服务器的内存：flume_env.sh  4-6g内存
            b）找兄弟，增加Flume服务器，对应增加日志服务器
                （日志服务器配置：内存8g/16g/32g磁盘可以大一点 8t/16t磁盘  比较便宜）
                （日志服务器有的公司计算到大数据集群，有的不算。 建议不算）     
</code></pre>
<h4 id="优化">优化</h4>
<pre><code>    1）HDFS sink
    2）put或take尝试提交的次数远远大于最终成功的次数
    3）filechannel 存储数据多目录，存储到不同的磁盘，可以增加吞吐量
    4）flume挂了有什么影响
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[zookeeper]]></title>
        <id>https://liuwenlu12.github.io//post/zookeeper</id>
        <link href="https://liuwenlu12.github.io//post/zookeeper">
        </link>
        <updated>2020-04-12T16:17:40.000Z</updated>
        <content type="html"><![CDATA[<h4 id="选举机制">选举机制</h4>
<pre><code>    半数机制
</code></pre>
<h4 id="常用命令">常用命令</h4>
<pre><code>    ls  get  create
</code></pre>
<h4 id="生产环境安装多少台">生产环境安装多少台</h4>
<pre><code>    安装奇数台：3  5  7   9  11
    10台服务器 ：3台 
    20台服务器： 5台
    100台服务器：11台
    不是越多越好  也不是越少越好
    台数少，影响可靠性；  台数多，会影响通信时间</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hadoop]]></title>
        <id>https://liuwenlu12.github.io//post/hadoop</id>
        <link href="https://liuwenlu12.github.io//post/hadoop">
        </link>
        <updated>2020-04-11T00:58:07.000Z</updated>
        <content type="html"><![CDATA[<h4 id="入门">入门</h4>
<p>常用端口号<br>
50070 19888 8088 9000 50090<br>
HDFS HOSTORY MR 客户端访问<br>
安装过程中8个配置文件<br>
common HDFS YARN MR<br>
core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml<br>
3env<br>
hadoop-env.sh hdfs-env.sh mapred-env.sh<br>
slaves (不能有空格 不能有空行)</p>
<h4 id="hdfs">HDFS</h4>
<p>HDFS读写流程</p>
<p>HDFS小文件<br>
什么情况会产生小文件<br>
日志文件=&gt;flume=&gt;kafka=&gt;flume(时间1-2小时、大小128m、event个数)=&gt;hdfs<br>
小文件影响什么<br>
增加namenode存储空间<br>
增加maotask数量、耗费过多内存资源<br>
1kb 1kb 1kb 3个maptask 耗费3g  1g<br>
增加JVM虚拟机开关次数<br>
怎么解决<br>
har/自定义inputformat<br>
7<em>150字节/1</em>150字节  seq</p>
<pre><code>    128gnamenode存储最多多少文件快 1个文件块150字节
    128g/150字节=128*1024m*1024kb*1024bit/150

    combineInputFormat
    减少切片的个数、进而减少maptask个数

    开启JVM重用
    如果没有小文件 不要开启JVM重用
</code></pre>
<h4 id="mapreduce">Mapreduce</h4>
<pre><code>shuffle及其优化
    ![](https://liuwenlu12.github.io//post-images/1586696107439.png)
    1. map方法之后，reduce方法之前的混洗过程       
    2. 溢写时排序 手段：快排 对谁排序：key的索引  按照什么规律：字典顺序
    3. 进行了那些排序  map阶段：快排 归并  reduce阶段 归并 分组
    4. 压缩 
        1.  map输入端：文件大小(lzo需要自己创建索引\bzip2)（如果文件大于128m 要考虑是否切片 如果文件小于128m 小文件处理）
        2. map输出端 考虑数据块  lzo snappy
        3. reduce输出端：看需求  最终数据去哪？ 保存考虑压缩 下个map输入考虑是否切片
  namenode默认内存8g  单任务内存8g  maptask默认内存1g reducetask默认1g 
</code></pre>
<h4 id="yarn">YARN</h4>
<p>1）yarn工作机制<br>
<img src="https://liuwenlu12.github.io//post-images/1586696325666.png" alt=""><br>
2）调度器<br>
（1）FIFO、容量、公平<br>
（2）默认调度器是哪个？<br>
Apache是容量<br>
CDH是公平<br>
（3）FIFO特点：<br>
单队列、先进先出<br>
（4）容量调度器特点：<br>
支持多队列、由多个FIFO调度器组成。先进先出、多队列之间相互借用资源<br>
（5）公平调度器特点<br>
支持多队列、每个队列公平享有资源。<br>
3）容量调度器默认几个队列<br>
就一个default队列<br>
4）生产环境如何设置队列<br>
（1）按照框架 hive  spark  flink<br>
（2）按照业务模块（用户行为、业务数据）<br>
（再细分：订单、购物车、点赞、评论、收藏、退款）<br>
解耦，降低风险。  递归死循环（退款）<br>
队列分等级（重点业务：订单、购物车。 普通业务：点赞、评论、收藏）</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[linux、shell]]></title>
        <id>https://liuwenlu12.github.io//post/fu-xi</id>
        <link href="https://liuwenlu12.github.io//post/fu-xi">
        </link>
        <updated>2020-04-11T00:40:57.000Z</updated>
        <content type="html"><![CDATA[<h4 id="linux">linux</h4>
<p>常用高级命令<br>
netstat、top、iotop、crotab、df -h、tar<br>
查看磁盘、查看进程、定时任务、查看端口<br>
df -h top ps crotab netstat</p>
<h4 id="shell">shell</h4>
<p>工具<br>
cut、awk、sed、sort<br>
哪些shell脚本</p>
<ol>
<li>框架启动停止脚本<pre><code>case $1 in
&quot;start&quot;)
     for i in hadoop102 hadoop103 hadoop104
     do
         ssh $i &quot;绝对路径&quot;
         done
</code></pre>
</li>
<li>数仓和mysql导入导出脚本 sqoop</li>
<li>数仓内部层级之间导入导出<pre><code>#!/bin/bash
Apache经常用 CDH不会用
App =gamll
hive=/opt/moudule/hive/bin/hive
时间
if[-n &quot;$1&quot;]
    do_date=$1
else
    do_date=`date -d &quot;-1 day&quot; +%F`
if
sql=&quot;遇到表在前面加上${APP}
    遇到时间替换为$do_date
&quot;
$hive -e &quot;$sql&quot;
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink任务链]]></title>
        <id>https://liuwenlu12.github.io//post/flink-ren-wu-lian</id>
        <link href="https://liuwenlu12.github.io//post/flink-ren-wu-lian">
        </link>
        <updated>2020-03-13T17:34:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="解析">解析</h3>
<p>合并的前提<br>
one-to-one 并且并行度相同</p>
<p>rebalance  两个任务并行度不同采用轮询  比如前一个任务并行度2 后一个3  轮询发送数据</p>
<p>类shuffle</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[电影：幸福终点站]]></title>
        <id>https://liuwenlu12.github.io//post/dian-ying-xing-fu-zhong-dian-zhan</id>
        <link href="https://liuwenlu12.github.io//post/dian-ying-xing-fu-zhong-dian-zhan">
        </link>
        <updated>2020-03-09T13:19:39.000Z</updated>
        <content type="html"><![CDATA[<p>自己的信念，</p>
<p>然后一直坚持，</p>
<p>全世界都会为你让路的。</p>
<p>善待他人，用心学习。</p>
<p>所以，坚持你所坚持的，</p>
<p>慢慢地去努力，</p>
<p>至于结果，</p>
<p>就让给时间吧。</p>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1583760337714.jpg" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark Shuffle 解析]]></title>
        <id>https://liuwenlu12.github.io//post/spark-shuffle-jie-xi</id>
        <link href="https://liuwenlu12.github.io//post/spark-shuffle-jie-xi">
        </link>
        <updated>2020-02-27T17:07:56.000Z</updated>
        <content type="html"><![CDATA[<h4 id="未优化的hashshuffle">未优化的HashShuffle</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582823385827.png" alt=""></figure>
<p>缺点:</p>
<ol>
<li>map 任务的中间结果首先存入内存(缓存), 然后才写入磁盘. 这对于内存的开销很大, 当一个节点上 map 任务的输出结果集很大时, 很容易导致内存紧张, 发生 OOM</li>
<li>生成很多的小文件. 假设有 M 个 MapTask, 有 N 个 ReduceTask, 则会创建 M * n 个小文件, 磁盘 I/O 将成为性能瓶颈.</li>
</ol>
<h4 id="优化的hashshuffle">优化的HashShuffle</h4>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582823456943.png" alt=""></figure>
<p>优化的 HashShuffle 过程就是启用合并机制，合并机制就是复用buffer，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p>
<h4 id="sortshuffle-解析">SortShuffle 解析</h4>
<p>普通 SortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823873456.png" alt=""></p>
<p>bypassSortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823904849.png" alt=""></p>
<p>bypass运行机制的触发条件如下(必须同时满足)：</p>
<ol>
<li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，默认为200。</li>
<li>不是聚合类的shuffle算子(没有预聚合)（比如groupByKey）。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark 任务调度机制]]></title>
        <id>https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi</id>
        <link href="https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi">
        </link>
        <updated>2020-02-27T14:44:47.000Z</updated>
        <content type="html"><![CDATA[<p>当 Driver 起来后，Driver 则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务。<br>
在详细阐述任务调度前，首先说明下 Spark 里的几个概念。一个 Spark 应用程序包括Job、Stage以及Task三个概念：</p>
<ol>
<li>Job 是以 Action 算子为界，遇到一个Action算子则触发一个Job；</li>
<li>Stage 是 Job 的子集，以 RDD 宽依赖(即 Shuffle )为界，遇到 Shuffle 做一次划分；</li>
<li>Task 是 Stage 的子集，以并行度(分区数)来衡量，这个 Stage 分区数是多少，则这个Stage 就有多少个 Task。<br>
Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度，一路是 Task 级的调度，总体调度流程如下图所示：</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582816815462.png" alt=""></figure>
<p>Spark RDD 通过其 Transactions 操作，形成了RDD血缘关系图，即DAG，最后通过Action的调用，触发Job并调度执行。<br>
DAGScheduler负责Stage级的调度，主要是将job切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度。<br>
TaskScheduler负责Task级的调度，将DAGScheduler传过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统。</p>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582821511401.png" alt=""></figure>
<p>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动 SchedulerBackend以及HeartbeatReceiver。<br>
SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。<br>
HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</p>
<h3 id="spark-stage-级别调度">Spark Stage 级别调度</h3>
<figure data-type="image" tabindex="3"><img src="https://liuwenlu12.github.io//post-images/1582821547270.png" alt=""></figure>
<ol>
<li>Job 由最终的RDD和Action方法封装而成；</li>
<li>SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是宽依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。</li>
<li>划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据</li>
</ol>
<h3 id="spark-task-级别调度">Spark Task 级别调度</h3>
<p>TaskSetManager结构如下图所示。<br>
<img src="https://liuwenlu12.github.io//post-images/1582821601280.png" alt=""></p>
<figure data-type="image" tabindex="4"><img src="https://liuwenlu12.github.io//post-images/1582821628549.png" alt=""></figure>
<h3 id="fifo调度策略">FIFO调度策略</h3>
<p>如果是采用FIFO调度策略，则直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构如下图所示，TaskSetManager保存在一个FIFO队列中。</p>
<figure data-type="image" tabindex="5"><img src="https://liuwenlu12.github.io//post-images/1582821681915.png" alt=""></figure>
<h3 id="fair调度策略08-开始支持">FAIR调度策略(0.8 开始支持)</h3>
<figure data-type="image" tabindex="6"><img src="https://liuwenlu12.github.io//post-images/1582821708143.png" alt=""></figure>
<p>FAIR模式中有一个rootPool和多个子Pool，各个子Pool中存储着所有待分配的TaskSetMagager。<br>
在FAIR模式中，需要先对子Pool进行排序，再对子Pool里面的TaskSetMagager进行排序，因为Pool和TaskSetMagager都继承了Schedulable特质，因此使用相同的排序算法。<br>
排序过程的比较是基于Fair-share来比较的，每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值、weight值，比较时会综合考量runningTasks值，minShare值以及weight值。<br>
注意，minShare、weight的值均在公平调度配置文件fairscheduler.xml中被指定，调度池在构建阶段会读取此文件的相关配置。</p>
<ol>
<li>如果 A 对象的runningTasks大于它的minShare，B 对象的runningTasks小于它的minShare，那么B排在A前面；（runningTasks 比 minShare 小的先执行）</li>
<li>如果A、B对象的 runningTasks 都小于它们的 minShare，那么就比较 runningTasks 与 math.max(minShare1, 1.0) 的比值（minShare使用率），谁小谁排前面；（minShare使用率低的先执行）</li>
<li>如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值（权重使用率），谁小谁排前面。（权重使用率低的先执行）</li>
<li>如果上述比较均相等，则比较名字。</li>
</ol>
<p>整体上来说就是通过minShare和weight这两个参数控制比较过程，可以做到让minShare使用率和权重使用率少（实际运行task比例较少）的先运行。<br>
FAIR模式排序完成后，所有的TaskSetManager被放入一个ArrayBuffer里，之后依次被取出并发送给Executor执行。<br>
从调度队列中拿到TaskSetManager后，由于TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。<br>
如何启用公平调度器:</p>
<pre><code>val conf = new SparkConf().setMaster(...).setAppName(...)
conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)
val sc = new SparkContext(conf)
</code></pre>
<h3 id="本地化调度">本地化调度</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>解析</th>
</tr>
</thead>
<tbody>
<tr>
<td>PROCESS_LOCAL</td>
<td>进程本地化，task和数据在同一个Executor中，性能最好。</td>
</tr>
<tr>
<td>NODE_LOCAL</td>
<td>节点本地化，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。</td>
</tr>
<tr>
<td>RACK_LOCAL</td>
<td>机架本地化，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td>
</tr>
<tr>
<td>NO_PREF</td>
<td>对于task来说，从哪里获取都一样，没有好坏之分。</td>
</tr>
<tr>
<td>ANY</td>
<td>task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。</td>
</tr>
</tbody>
</table>
<h3 id="失败重试和黑名单">失败重试和黑名单</h3>
<p>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，<br>
“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了。</p>
]]></content>
    </entry>
</feed>