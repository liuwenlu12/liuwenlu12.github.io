<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liuwenlu12.github.io/</id>
    <title>刘文路的博客</title>
    <updated>2020-03-15T12:31:34.830Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liuwenlu12.github.io/"/>
    <link rel="self" href="https://liuwenlu12.github.io//atom.xml"/>
    <subtitle>与其仓皇追赶日落，不如静待漫天星辰</subtitle>
    <logo>https://liuwenlu12.github.io//images/avatar.png</logo>
    <icon>https://liuwenlu12.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, 刘文路的博客</rights>
    <entry>
        <title type="html"><![CDATA[Flink任务链]]></title>
        <id>https://liuwenlu12.github.io//post/flink-ren-wu-lian</id>
        <link href="https://liuwenlu12.github.io//post/flink-ren-wu-lian">
        </link>
        <updated>2020-03-13T17:34:05.000Z</updated>
        <content type="html"><![CDATA[<h3 id="解析">解析</h3>
<p>合并的前提<br>
one-to-one 并且并行度相同</p>
<p>rebalance  两个任务并行度不同采用轮询  比如前一个任务并行度2 后一个3  轮询发送数据</p>
<p>类shuffle</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[电影：幸福终点站]]></title>
        <id>https://liuwenlu12.github.io//post/dian-ying-xing-fu-zhong-dian-zhan</id>
        <link href="https://liuwenlu12.github.io//post/dian-ying-xing-fu-zhong-dian-zhan">
        </link>
        <updated>2020-03-09T13:19:39.000Z</updated>
        <content type="html"><![CDATA[<p>自己的信念，</p>
<p>然后一直坚持，</p>
<p>全世界都会为你让路的。</p>
<p>善待他人，用心学习。</p>
<p>所以，坚持你所坚持的，</p>
<p>慢慢地去努力，</p>
<p>至于结果，</p>
<p>就让给时间吧。</p>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1583760337714.jpg" alt=""></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark Shuffle 解析]]></title>
        <id>https://liuwenlu12.github.io//post/spark-shuffle-jie-xi</id>
        <link href="https://liuwenlu12.github.io//post/spark-shuffle-jie-xi">
        </link>
        <updated>2020-02-27T17:07:56.000Z</updated>
        <content type="html"><![CDATA[<h4 id="未优化的hashshuffle">未优化的HashShuffle</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582823385827.png" alt=""></figure>
<p>缺点:</p>
<ol>
<li>map 任务的中间结果首先存入内存(缓存), 然后才写入磁盘. 这对于内存的开销很大, 当一个节点上 map 任务的输出结果集很大时, 很容易导致内存紧张, 发生 OOM</li>
<li>生成很多的小文件. 假设有 M 个 MapTask, 有 N 个 ReduceTask, 则会创建 M * n 个小文件, 磁盘 I/O 将成为性能瓶颈.</li>
</ol>
<h4 id="优化的hashshuffle">优化的HashShuffle</h4>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582823456943.png" alt=""></figure>
<p>优化的 HashShuffle 过程就是启用合并机制，合并机制就是复用buffer，开启合并机制的配置是spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p>
<h4 id="sortshuffle-解析">SortShuffle 解析</h4>
<p>普通 SortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823873456.png" alt=""></p>
<p>bypassSortShuffle<br>
<img src="https://liuwenlu12.github.io//post-images/1582823904849.png" alt=""></p>
<p>bypass运行机制的触发条件如下(必须同时满足)：</p>
<ol>
<li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，默认为200。</li>
<li>不是聚合类的shuffle算子(没有预聚合)（比如groupByKey）。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark 任务调度机制]]></title>
        <id>https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi</id>
        <link href="https://liuwenlu12.github.io//post/spark-ren-wu-diao-du-ji-zhi">
        </link>
        <updated>2020-02-27T14:44:47.000Z</updated>
        <content type="html"><![CDATA[<p>当 Driver 起来后，Driver 则会根据用户程序逻辑准备任务，并根据Executor资源情况逐步分发任务。<br>
在详细阐述任务调度前，首先说明下 Spark 里的几个概念。一个 Spark 应用程序包括Job、Stage以及Task三个概念：</p>
<ol>
<li>Job 是以 Action 算子为界，遇到一个Action算子则触发一个Job；</li>
<li>Stage 是 Job 的子集，以 RDD 宽依赖(即 Shuffle )为界，遇到 Shuffle 做一次划分；</li>
<li>Task 是 Stage 的子集，以并行度(分区数)来衡量，这个 Stage 分区数是多少，则这个Stage 就有多少个 Task。<br>
Spark 的任务调度总体来说分两路进行，一路是 Stage 级的调度，一路是 Task 级的调度，总体调度流程如下图所示：</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582816815462.png" alt=""></figure>
<p>Spark RDD 通过其 Transactions 操作，形成了RDD血缘关系图，即DAG，最后通过Action的调用，触发Job并调度执行。<br>
DAGScheduler负责Stage级的调度，主要是将job切分成若干Stages，并将每个Stage打包成TaskSet交给TaskScheduler调度。<br>
TaskScheduler负责Task级的调度，将DAGScheduler传过来的TaskSet按照指定的调度策略分发到Executor上执行，调度过程中SchedulerBackend负责提供可用资源，其中SchedulerBackend有多种实现，分别对接不同的资源管理系统。</p>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582821511401.png" alt=""></figure>
<p>Driver初始化SparkContext过程中，会分别初始化DAGScheduler、TaskScheduler、SchedulerBackend以及HeartbeatReceiver，并启动 SchedulerBackend以及HeartbeatReceiver。<br>
SchedulerBackend通过ApplicationMaster申请资源，并不断从TaskScheduler中拿到合适的Task分发到Executor执行。<br>
HeartbeatReceiver负责接收Executor的心跳信息，监控Executor的存活状况，并通知到TaskScheduler。</p>
<h3 id="spark-stage-级别调度">Spark Stage 级别调度</h3>
<figure data-type="image" tabindex="3"><img src="https://liuwenlu12.github.io//post-images/1582821547270.png" alt=""></figure>
<ol>
<li>Job 由最终的RDD和Action方法封装而成；</li>
<li>SparkContext将Job交给DAGScheduler提交，它会根据RDD的血缘关系构成的DAG进行切分，将一个Job划分为若干Stages，具体划分策略是，由最终的RDD不断通过依赖回溯判断父依赖是否是宽依赖，即以Shuffle为界，划分Stage，窄依赖的RDD之间被划分到同一个Stage中，可以进行pipeline式的计算。</li>
<li>划分的Stages分两类，一类叫做ResultStage，为DAG最下游的Stage，由Action方法决定，另一类叫做ShuffleMapStage，为下游Stage准备数据</li>
</ol>
<h3 id="spark-task-级别调度">Spark Task 级别调度</h3>
<p>TaskSetManager结构如下图所示。<br>
<img src="https://liuwenlu12.github.io//post-images/1582821601280.png" alt=""></p>
<figure data-type="image" tabindex="4"><img src="https://liuwenlu12.github.io//post-images/1582821628549.png" alt=""></figure>
<h3 id="fifo调度策略">FIFO调度策略</h3>
<p>如果是采用FIFO调度策略，则直接简单地将TaskSetManager按照先来先到的方式入队，出队时直接拿出最先进队的TaskSetManager，其树结构如下图所示，TaskSetManager保存在一个FIFO队列中。</p>
<figure data-type="image" tabindex="5"><img src="https://liuwenlu12.github.io//post-images/1582821681915.png" alt=""></figure>
<h3 id="fair调度策略08-开始支持">FAIR调度策略(0.8 开始支持)</h3>
<figure data-type="image" tabindex="6"><img src="https://liuwenlu12.github.io//post-images/1582821708143.png" alt=""></figure>
<p>FAIR模式中有一个rootPool和多个子Pool，各个子Pool中存储着所有待分配的TaskSetMagager。<br>
在FAIR模式中，需要先对子Pool进行排序，再对子Pool里面的TaskSetMagager进行排序，因为Pool和TaskSetMagager都继承了Schedulable特质，因此使用相同的排序算法。<br>
排序过程的比较是基于Fair-share来比较的，每个要排序的对象包含三个属性: runningTasks值（正在运行的Task数）、minShare值、weight值，比较时会综合考量runningTasks值，minShare值以及weight值。<br>
注意，minShare、weight的值均在公平调度配置文件fairscheduler.xml中被指定，调度池在构建阶段会读取此文件的相关配置。</p>
<ol>
<li>如果 A 对象的runningTasks大于它的minShare，B 对象的runningTasks小于它的minShare，那么B排在A前面；（runningTasks 比 minShare 小的先执行）</li>
<li>如果A、B对象的 runningTasks 都小于它们的 minShare，那么就比较 runningTasks 与 math.max(minShare1, 1.0) 的比值（minShare使用率），谁小谁排前面；（minShare使用率低的先执行）</li>
<li>如果A、B对象的runningTasks都大于它们的minShare，那么就比较runningTasks与weight的比值（权重使用率），谁小谁排前面。（权重使用率低的先执行）</li>
<li>如果上述比较均相等，则比较名字。</li>
</ol>
<p>整体上来说就是通过minShare和weight这两个参数控制比较过程，可以做到让minShare使用率和权重使用率少（实际运行task比例较少）的先运行。<br>
FAIR模式排序完成后，所有的TaskSetManager被放入一个ArrayBuffer里，之后依次被取出并发送给Executor执行。<br>
从调度队列中拿到TaskSetManager后，由于TaskSetManager封装了一个Stage的所有Task，并负责管理调度这些Task，那么接下来的工作就是TaskSetManager按照一定的规则一个个取出Task给TaskScheduler，TaskScheduler再交给SchedulerBackend去发到Executor上执行。<br>
如何启用公平调度器:</p>
<pre><code>val conf = new SparkConf().setMaster(...).setAppName(...)
conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)
val sc = new SparkContext(conf)
</code></pre>
<h3 id="本地化调度">本地化调度</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>解析</th>
</tr>
</thead>
<tbody>
<tr>
<td>PROCESS_LOCAL</td>
<td>进程本地化，task和数据在同一个Executor中，性能最好。</td>
</tr>
<tr>
<td>NODE_LOCAL</td>
<td>节点本地化，task和数据在同一个节点中，但是task和数据不在同一个Executor中，数据需要在进程间进行传输。</td>
</tr>
<tr>
<td>RACK_LOCAL</td>
<td>机架本地化，task和数据在同一个机架的两个节点上，数据需要通过网络在节点之间进行传输。</td>
</tr>
<tr>
<td>NO_PREF</td>
<td>对于task来说，从哪里获取都一样，没有好坏之分。</td>
</tr>
<tr>
<td>ANY</td>
<td>task和数据可以在集群的任何地方，而且不在一个机架中，性能最差。</td>
</tr>
</tbody>
</table>
<h3 id="失败重试和黑名单">失败重试和黑名单</h3>
<p>在记录Task失败次数过程中，会记录它上一次失败所在的Executor Id和Host，这样下次再调度这个Task时，会使用黑名单机制，避免它被调度到上一次失败的节点上，起到一定的容错作用。黑名单记录Task上一次失败所在的Executor Id和Host，以及其对应的“拉黑”时间，<br>
“拉黑”时间是指这段时间内不要再往这个节点上调度这个Task了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark 部署模式及源码分析]]></title>
        <id>https://liuwenlu12.github.io//post/spark-bu-shu-mo-shi</id>
        <link href="https://liuwenlu12.github.io//post/spark-bu-shu-mo-shi">
        </link>
        <updated>2020-02-27T08:13:12.000Z</updated>
        <content type="html"><![CDATA[<p>Spark支持3种集群管理器（Cluster Manager），分别为：</p>
<ol>
<li>Standalone：独立模式，Spark 原生的简单集群管理器，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统，使用 Standalone 可以很方便地搭建一个集群；</li>
<li>Hadoop YARN：统一的资源管理机制，在上面可以运行多套计算框架，如 MR、Storm等。根据 Driver 在集群中的位置不同，分为 yarn client 和 yarn cluster；</li>
<li>Apache Mesos：一个强大的分布式资源管理框架，它允许多种不同的框架部署在其上，包括 Yarn。</li>
</ol>
<h3 id="yarn-cluster-模式">YARN Cluster 模式</h3>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582794556881.png" alt=""></figure>
<ol>
<li>执行脚本提交任务，实际是启动一个 SparkSubmit 的 JVM 进程；</li>
<li>SparkSubmit 类中的 main方法反射调用Client的main方法；</li>
<li>Client创建Yarn客户端，然后向Yarn发送执行指令：bin/java ApplicationMaster；</li>
<li>Yarn框架收到指令后会在指定的NM中启动ApplicationMaster；</li>
<li>ApplicationMaster启动Driver线程，执行用户的作业；</li>
<li>AM向RM注册，申请资源；</li>
<li>获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBacken；</li>
<li>启动ExecutorBackend, 并向driver注册.</li>
<li>注册成功后, ExecutorBackend会创建一个Executor对象.</li>
<li>Driver会给ExecutorBackend分配任务, 并监控任务的执行.<br>
注意:<br>
•	SparkSubmit、ApplicationMaster和CoarseGrainedExecutorBacken是独立的进程；<br>
•	Client和Driver是独立的线程；<br>
•	Executor是一个对象。</li>
</ol>
<pre><code>cluster: 
bin/spark-submit \                                             
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode cluster \
./examples/jars/spark-examples_2.11-2.1.1.jar 1000

-------
SparkSubmit
ApplicationMaster
CoarseGrainedExecutorBackend
--------

SparkSubmit
    org.apache.spark.deploy.SparkSubmit


    if (deployMode == CLIENT || isYarnCluster) {
            // 如果是客户端模式, childMainClass 就是用户的类
            // 集群模式下, childMainClass 被重新赋值为 org.apache.spark.deploy.yarn.Client
            childMainClass = args.mainClass  // 用户主类
            if (isUserJar(args.primaryResource)) {
                childClasspath += args.primaryResource
            }
            if (args.jars != null) {
                childClasspath ++= args.jars.split(&quot;,&quot;)
            }
        }
    if (isYarnCluster) {
            // 在 yarn 集群模式下, 使用yarn.Client来封装一下 user class
            childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
            if (args.isPython) {
                childArgs += (&quot;--primary-py-file&quot;, args.primaryResource)
                childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.PythonRunner&quot;)
            } else if (args.isR) {
                val mainFile = new Path(args.primaryResource).getName
                childArgs += (&quot;--primary-r-file&quot;, mainFile)
                childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.RRunner&quot;)
            } else {
                if (args.primaryResource != SparkLauncher.NO_RESOURCE) {
                    childArgs += (&quot;--jar&quot;, args.primaryResource)
                }
                childArgs += (&quot;--class&quot;, args.mainClass)
            }
            if (args.childArgs != null) {
                args.childArgs.foreach { arg =&gt; childArgs += (&quot;--arg&quot;, arg) }
            }
        }

        childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
            submitApplication()

        // 核心代码: 确定 ApplicationMaster 类
        val amClass =
            if (isClusterMode) { // 如果是 cluster 模式
                Utils.classForName(&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;).getName
            } else { // 如果是 client 模式
                Utils.classForName(&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;).getName
            }

        yarnClient.submitApplication(appContext)

        spark-submit进程执行完毕

ApplicationMaster
        runDriver()
        注册AM 让NM要启动的类
        org.apache.spark.executor.CoarseGrainedExecutorBackend

        执行完毕, 但是AM并不会退出,  dirverThread.join()
CoarseGrainedExecutorBackend
        executor进程
        // 获取到 driver 的 RpcEndpointRef  (DriverEndpoint是在sparkContext中创建)
            val driver: RpcEndpointRef = fetcher.setupEndpointRefByURI(driverUrl)

        ref.ask[Boolean](RegisterExecutor(executorId, self, hostname, cores, extractLogUrls))

        driver端:
            executorRef.send(RegisteredExecutor)
        executor端:
            executor = new Executor(executorId, hostname, env, userClassPath, isLocal = false)

</code></pre>
<p><strong>Cluster 模式概述</strong>：脚本在客户端上提交执行之后，执行SparkSubmit的main方法，main方法中会反射一个cilent类，执行client的main方法，封装一个指令(bin/java Application)交给RM,RM会选择一台NodeManager启动AM,AM会运行Driver(运行用户类的main方法),同时AM会向RM申请资源和容器,封装并且发送一个指令(bin/java CoarseGrainedExecutorBackend),让其他NodeManager启动ExecutorBackend进程,ExecutorBackend进程会向Driver进行反向注册(RegisterExecutor),Driver返回一个成功注册的信息,Executor进程会创建一个Executor对象,之后就可以进行任务分配</p>
<p><strong>SparkSubmit  Application ExecutorBackend三个进程</strong></p>
<h3 id="yarn-client-模式">Yarn Client 模式</h3>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582800089613.png" alt=""></figure>
<ol>
<li>执行脚本提交任务，实际是启动一个SparkSubmit的 JVM 进程；</li>
<li>SparkSubmit伴生对象中的main方法反射调用用户代码的main方法；</li>
<li>启动Driver线程，执行用户的作业，并创建ScheduleBackend；</li>
<li>YarnClientSchedulerBackend向RM发送指令：bin/java ExecutorLauncher；</li>
<li>Yarn框架收到指令后会在指定的NM中启动ExecutorLauncher（实际上还是调用ApplicationMaster的main方法）；</li>
</ol>
<pre><code>object ExecutorLauncher {
  def main(args: Array[String]): Unit = {
    ApplicationMaster.main(args)
  }
}
</code></pre>
<ol start="6">
<li>AM向RM注册，申请资源；</li>
<li>获取资源后AM向NM发送指令：bin/java CoarseGrainedExecutorBacken；</li>
<li>后面和cluster模式一致<br>
注意：</li>
</ol>
<ul>
<li>SparkSubmit、ExecutorLauncher和CoarseGrainedExecutorBacken是独立的进程；</li>
<li>driver不是一个子线程,而是直接运行在SparkSubmit进程的main线程中, 所以sparkSubmit进程不能退出.</li>
</ul>
<pre><code>bin/spark-submit \                                            
--class org.apache.spark.examples.SparkPi \
--master yarn \
--deploy-mode client \
./examples/jars/spark-examples_2.11-2.1.1.jar 1000

------
SparkSubmit
ExecutorLauncher  (am)
CoarseGrainedExecutorBackend

SparkSubmit
    childMainClass = args.mainClass  // 用户主类

    执行用户类的main方法(执行driver)
        是在SparkSubmit的主线程中执行
        (cluster 模式driver跟着: am, am启动了一个子线程)

    用户类里面, 首先就是在创建和初始化SaprkContext

    SparkContext的初始化:
        val (sched, ts): (SchedulerBackend, TaskScheduler) = SparkContext.createTaskScheduler(this, master, deployMode)
        
        _ts.start()
            backend.start()  
            CoarseGrainedSchedulerBackend: start
                driverEndpoint = createDriverEndpointRef(properties)

            client.submitApplication

                org.apache.spark.deploy.yarn.ExecutorLauncher
                为什么要换个名字：目的是为了区分是运行的client还是cluster模式

            启动DrvierEndPoint
                makeOffers
                    launchTasks(scheduler.resourceOffers(workOffers))

</code></pre>
<p><strong>Client模式概述</strong>：脚本submit提交，脚本启动，执行SparkSubmit的main方法(同时Driver也在SparkSubmit主线程中启动)，Driver的执行会初始化SchedulerBackend, TaskScheduler，其中的YarnCilentSchedulerBackend中有个start方法封装一个bin/java ExcutorLauncher指令交给RM,RM会找一个NodeManager执行ExcutorLauncher(AM),ExcutorLauncher启动后会向RM申请资源，容器.AM封装发送指令， 然后会根据资源和容器去分配NodeManager启动ExcutorBackend进程，ExcutorBackend进程会向进行Driver注册(使用RegisterExecutor),返回注册成功之后，创建一个Executor对象，就可以分配任务。</p>
<h3 id="client模式-和-cluster模式的区别">Client模式 和 Cluster模式的区别</h3>
<pre><code>1. spark-submit中反射childMainClass不同
    cluster:Client    childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot;
    client:用户类      childMainClass = args.mainClass
2. driver位置
    cluster:位于ApplicationMaster进程中，起一个子线程
    client:位于SparkSubmit的主线程
3. 启动AM
    cluster:ApplicationMaster
    client:ExecutorLauncher

  在cluster模式下，如果AM一旦启动，SparkSubmit还有用吗？   没有用 可以杀掉 一直在打印日志
  在client模式下 ，不能杀  杀了Driver也没了
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[笔记]]></title>
        <id>https://liuwenlu12.github.io//post/bi-ji</id>
        <link href="https://liuwenlu12.github.io//post/bi-ji">
        </link>
        <updated>2020-02-26T16:08:30.000Z</updated>
        <content type="html"><![CDATA[<p>···<br>
saveAsTable:<br>
1. 如果表不存在, 则会自动创建<br>
2. 如果表存在(append),  则后面存储的数据, 元数据的顺序要与表中的元数据的顺序一致<br>
表内数据:<br>
a(int)     b(string)<br>
新的数据:<br>
aa(int)    bb(string)<br>
3. 列名可以不一致</p>
<p>insertInto<br>
1. 要求表必须存在<br>
2. 要求列名必须一致.<br>
表内数据:<br>
a(int)  b(String)</p>
<pre><code>    新的数据:
        b(string)  a(int)
</code></pre>
<p>···</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ Spark集群启动流程分析]]></title>
        <id>https://liuwenlu12.github.io//post/spark-ji-qun-qi-dong-liu-cheng-fen-xi</id>
        <link href="https://liuwenlu12.github.io//post/spark-ji-qun-qi-dong-liu-cheng-fen-xi">
        </link>
        <updated>2020-02-26T15:53:46.000Z</updated>
        <content type="html"><![CDATA[<h4 id="流程">流程</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582732625980.png" alt=""></figure>
<h4 id="master服务端口-7077">Master服务端口 7077</h4>
<pre><code>   1 val args = new MasterArguments(argStrings, conf)  封装参数

  2  val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)   启动RPC，Endpoint

 3 val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) 创建rpcEnv

4     new NettyRpcEnvFactory().create(config) //Netty

5    if (!config.clientMode) //集群模式为true

6           Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1  启动服务NettyRPCEnv

7           val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME,
      new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) 启动Endpoint

8      dispatcher.registerRpcEndpoint(name, endpoint) //注册到dispatcher
            消息分发器 提升异步能力
        
---------------------------------------------
private[spark] trait RpcEnvFactory {
 * An end point for the RPC that defines what functions to trigger given a message.
 *
 * It is guaranteed that `onStart`, `receive` and `onStop` will be called in sequence.
 *
 * The life-cycle of an endpoint is:
 *
 * constructor -&gt; onStart -&gt; receive* -&gt; onStop

* Note: `receive` can be called concurrently. If you want `receive` to be thread-safe, please use
 * [[ThreadSafeRpcEndpoint]]
 *
 * If any error is thrown from one of [[RpcEndpoint]] methods except `onError`, `onError` will be
 * invoked with the cause. If `onError` throws an error, [[RpcEnv]] will ignore it.
---------------------------------------------
 9 onstart --&gt;
 checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate//固定检查worker是否超时  默认每分钟一次  自己给自己发消息         self.send(CheckForWorkerTimeOut)
 10 receive--&gt;
  override def receive: PartialFunction[Any, Unit] 
     case CheckForWorkerTimeOut =&gt;
      timeOutDeadWorkers()//移除worker
</code></pre>
<h4 id="worker">Worker</h4>
<pre><code> 1 val args = new WorkerArguments(argStrings, conf)  封装参数
    
 2 val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,
      args.memory, args.masters, args.workDir, conf = conf) 启动RPC，Endpoint

  val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr) 创建rpcEnv

 val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_))  Mster地址(注册用  高可用有多个  都发送) 

 rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory,
      masterAddresses, ENDPOINT_NAME, workDir, conf, securityMgr))   setupEndpoint

3 new Worker--&gt;

def assert(assertion: Boolean) {
    if (!assertion)
      throw new java.lang.AssertionError(&quot;assertion failed&quot;)  已注册抛异常
  }

  4  registerWithMaster()  //注册自己

    registerMasterFutures = tryRegisterAllMasters() //所有master注册

     val masterEndpoint = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)
     registerWithMaster(masterEndpoint)
        //获取ref 注册

  masterEndpoint.ask[RegisterWorkerResponse] RegisterWorker(
      workerId, host, port, self, cores, memory, workerWebUiUrl))
 //发信息 希望Mster回一RegisterWorkerResponse  给master自己所有信息

  master端   override def receiveAndReply(context: RpcCallContext): PartialFunction 
     if (state == RecoveryState.STANDBY) {
        context.reply(MasterInStandby)   //自己STANDBY
....各种交互
           5   if (registerWorker(worker)) //Master给与注册

   workers.filter { w =&gt;
      (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == WorkerState.DEAD)
    }.foreach { w =&gt;
      workers -= w
    }   //曾经注册过 状态死  

  context.reply(RegisteredWorker(self, masterWebUiUrl))//响应worker 告诉worker我是哪个master


logInfo(&quot;Successfully registered with master &quot; + masterRef.address.toSparkURL)
      registered = true
     changeMaster(masterRef, masterWebUiUrl) //改地址
forwordMessageScheduler.scheduleAtFixedRate //发心跳

      if (connected) { sendToMaster(Heartbeat(workerId, self)) }  //心跳
      case Some(masterRef) =&gt; masterRef.send(message)

         workerInfo.lastHeartbeat = System.currentTimeMillis() //Master端 一分钟超时  十五秒心跳

master如果没起来   用定时器尝试继续注册自己  16次（6+10）

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark通讯架构]]></title>
        <id>https://liuwenlu12.github.io//post/spark-tong-xun-jia-gou</id>
        <link href="https://liuwenlu12.github.io//post/spark-tong-xun-jia-gou">
        </link>
        <updated>2020-02-26T15:18:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="spark-内置-rpc-框架">Spark 内置 RPC 框架</h4>
<p>在 Spark 中, 很多地方都涉及到网络通讯, 比如 Spark 各个组件间的消息互通, 用户文件与 Jar 包的上传, 节点间的 Shuffle 过程, Block 数据的复制与备份等.</p>
<ol>
<li>在 Spark0.x.x 与 Spark1.x.x 版本中, 组件间的消息通信主要借助于 Akka.</li>
<li>在 Spark1.3 中引入了 Netty 通信框架. Akka要求message发送端和接收端有相同的版本, 所以为了避免 Akka 造成的版本问题，并给用户的应用更大灵活性，决定使用更通用的 RPC 实现，也就是现在的 Netty 来替代 Akka。</li>
<li>Spark1.6 中 Akka 和 Netty 可以配置使用。Netty 完全实现了 Akka 在Spark 中的功能。</li>
<li>从Spark2.0.0, Akka 被移除.</li>
</ol>
<h4 id="actor-模型">Actor 模型</h4>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582730544913.png" alt=""></figure>
<h4 id="netty-通信架构">Netty 通信架构</h4>
<p>Netty 借鉴了 Akka 的 Actor 模型<br>
Spark通讯框架中各个组件（Client/Master/Worker）可以认为是一个个独立的实体，各个实体之间通过消息来进行通信。</p>
<figure data-type="image" tabindex="2"><img src="https://liuwenlu12.github.io//post-images/1582730593467.png" alt=""></figure>
<p>详细<br>
<img src="https://liuwenlu12.github.io//post-images/1582730972828.png" alt=""></p>
<p>高层俯视图<br>
<img src="https://liuwenlu12.github.io//post-images/1582731230217.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark内核]]></title>
        <id>https://liuwenlu12.github.io//post/spark-nei-he</id>
        <link href="https://liuwenlu12.github.io//post/spark-nei-he">
        </link>
        <updated>2020-02-26T14:43:04.000Z</updated>
        <content type="html"><![CDATA[<h3 id="spark核心组件">Spark核心组件</h3>
<ol>
<li>Cluster Manager (分配的资源属于一级分配, 它将各个 Worker 上的内存, CPU 等资源分配给 Application, 但并不负责对 Executor 的资源的分配)
<ol>
<li>Master  (Standalone)</li>
<li>ResourceManager (Yarn)</li>
<li>MesosMaster (Mesos)</li>
<li>Kubernetes  2.3.0新增 (docker配合)</li>
</ol>
</li>
<li>Worker (工作节点 在 Yarn 部署模式下实际由 NodeManager 替代)
<ol>
<li>将自己的内存, CPU 等资源通过注册机制告知 Cluster Manager</li>
<li>创建 Executor进程</li>
<li>将资源和任务进一步分配给 Executor</li>
<li>同步资源信息, Executor 状态信息给 ClusterManager 等.</li>
</ol>
</li>
<li>Driver (Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作)
<ol>
<li>将用户程序转化为作业（Job）；</li>
<li>在 Executor 之间调度任务（Task）；</li>
<li>跟踪 Executor 的执行情况；</li>
<li>通过 UI 展示查询运行情况；</li>
</ol>
</li>
<li>Executor (Spark Executor 节点是负责在 Spark 作业中运行具体任务，任务彼此之间相互独立)        Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。<br>
如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。<br>
Executor 有两个核心功能：</li>
<li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器（Driver）；</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 的数据是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
<li>Application<br>
• Application 通过 Spark API 将进行 RDD 的转换和 DAG 的构建, 并通过 Driver 将 Application 注册到 Cluster Manager.<br>
•	Cluster Manager 将会根据 Application 的资源需求, 通过一级分配将 Executor, 内存, CPU 等资源分配给 Application.<br>
•	Driver 通过二级分配将 Executor 等资源分配给每一个任务, Application 最后通过 Driver 告诉 Executor 运行任务</li>
</ol>
<h5 id="4040端口-driver">4040端口-&gt;Driver</h5>
<h5 id="8080端口-master">8080端口-&gt;Master</h5>
<h5 id="8081端口-worker">8081端口-&gt;Worker</h5>
<h3 id="spark-通用运行流程概述">Spark 通用运行流程概述</h3>
<figure data-type="image" tabindex="1"><img src="https://liuwenlu12.github.io//post-images/1582730179336.png" alt=""></figure>
<ol>
<li>任务提交后，都会先启动 Driver 程序；</li>
<li>随后 Driver 向集群管理器注册应用程序；</li>
<li>之后集群管理器根据此任务的配置文件分配 Executor 并启动该应用程序；</li>
<li>当 Driver 所需的资源全部满足后，Driver 开始执行 main 函数，Spark 转换为懒执行，当执行到 Action 算子时开始反向推算，根据宽依赖进行 Stage 的划分，随后每一个 Stage 对应一个 Taskset，Taskset 中有多个Task；</li>
<li>根据本地化原则，Task 会被分发到指定的 Executor 去执行，在任务执行的过程中，Executor 也会不断与 Driver 进行通信，报告任务运行情况。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkStreaming处理完数据写入redis]]></title>
        <id>https://liuwenlu12.github.io//post/sparkstreaming-chu-li-wan-shu-ju-xie-ru-redis</id>
        <link href="https://liuwenlu12.github.io//post/sparkstreaming-chu-li-wan-shu-ju-xie-ru-redis">
        </link>
        <updated>2020-02-26T14:27:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="redisutil">RedisUtil</h4>
<pre><code>package com.atguigu.day02.project.util

import redis.clients.jedis.Jedis

object RedisUtil {
  /**
   * 两种方式：
   * 1.使用连接池
   * 效率更高 连接会重用
   * 实际情况 容易出现多线程bug
   * 2.手动创建连接的客户端对象
   * 用完务必关闭
   */
  val host = &quot;lwl007&quot;
  val port = 6379
    //获取客户端对象
  def getClient = new Jedis(host, port)
    //关闭客户端
  def close(client: Jedis): Unit = {
    if (client != null) client.close()
  }
}

</code></pre>
<h4 id="数据写入">数据写入</h4>
<pre><code>    import org.json4s.JsonDSL._ //scala自带转json
    import scala.collection.JavaConversions._ //scala和java集合互转
    val key = &quot;last:hour:ads&quot;
    adsIdAndHmCountIt.foreachRDD(rdd =&gt; {
      rdd.foreachPartition((it: Iterator[(String, Iterable[(String, Int)])]) =&gt; {
        //写入数据
        if (it.hasNext) {
          //建立连接
          val client: Jedis = RedisUtil.getClient
          val map: Map[String, String] = it.map {
            case (adsId, hmCountIt) =&gt; {
              //不加toMap [{},{}]  加toMap{K1:V1,K2:V2}
              val hmCountItToMap: Map[String, Int] = hmCountIt.toMap
              (adsId, JsonMethods.compact(JsonMethods.render(hmCountItToMap)))
            }
          }.toMap
          client.hmset(key, map)
          RedisUtil.close(client)
        }
      })
    })
</code></pre>
]]></content>
    </entry>
</feed>